{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if it's your first time running spacy\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download zh_core_web_sm\n",
    "#!python -m spacy download xx_sent_ud_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import jieba\n",
    "import string \n",
    "import re \n",
    "from nltk.tokenize import MWETokenizer, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_pairs = [\n",
    "    'en-fi', 'en-zh', 'cs-en'\n",
    "    , 'de-en', 'ru-en', 'zh-en'\n",
    "]\n",
    "translations = ['fi', 'zh', 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fi = []\n",
    "corpus_zh = []\n",
    "corpus_en = []\n",
    "for pair in language_pairs:\n",
    "    languages = pair.split('-')\n",
    "    for i in range(2):\n",
    "        all_sentences = pd.read_csv(os.path.join('..', 'corpus',pair, 'scores.csv'))\n",
    "        if i == 0:\n",
    "            sentences = all_sentences['source']\n",
    "        if i == 1:\n",
    "            sentences = all_sentences['reference'].append(all_sentences['translation'], ignore_index=True)\n",
    "              \n",
    "        if languages[i]=='fi':\n",
    "            corpus_fi.append(sentences)\n",
    "        elif languages[i]=='zh':\n",
    "            corpus_zh.append(sentences)\n",
    "        elif languages[i]=='en':\n",
    "            corpus_en.append(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fi = list(pd.concat(corpus_fi))\n",
    "corpus_en = list(pd.concat(corpus_en))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(corpus, tool, language = 'en', pos_to_remove = ['PUNCT','NUM'], ent_to_remove = ['PERSON','ORG'], stop_words_to_remove= False, lowercase = True):\n",
    "    \"\"\"\n",
    "    tool: one of two strings - 'spacy' or 'NLTK'\n",
    "    languages (string ISO code): supports 'en', 'fi' or 'zh'\n",
    "    pos_to_remove (list): part-of-speech tag from spacy\n",
    "    ent_to_remove (list): entities from spacy\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    tokenized_corpus = []\n",
    "    if tool == 'spacy':\n",
    "        if language == 'en':\n",
    "            sc = spacy.load('en_core_web_sm')\n",
    "            for doc in tqdm(sc.pipe(corpus, disable=[\"lemmatizer\", \"textcat\", \"custom\"])):\n",
    "                if stop_words_to_remove:\n",
    "                    doc_list = [word.text for word in doc if word.pos_ not in pos_to_remove if word.ent_type_ not in ent_to_remove if word.is_stop]\n",
    "                else: \n",
    "                    doc_list = [word.text for word in doc if word.pos_ not in pos_to_remove if word.ent_type_ not in ent_to_remove]\n",
    "\n",
    "                if lowercase:\n",
    "                    doc_list = [word.lower() for word in doc_list]\n",
    "\n",
    "                tokenized_corpus.append(doc_list)\n",
    "        if language == 'zh':\n",
    "            sc = spacy.load('zh_core_web_sm')\n",
    "            for doc in tqdm(sc.pipe(corpus, disable=[\"lemmatizer\", \"textcat\", \"custom\"])):\n",
    "                if stop_words_to_remove:\n",
    "                    doc_list = [word.text for word in doc if word.pos_ not in ['PUNCT'] if word.is_stop]\n",
    "                else: \n",
    "                    doc_list = [word.text for word in doc if word.pos_ not in ['PUNCT']]\n",
    "                \n",
    "                tokenized_corpus.append(doc_list)\n",
    "        if language == 'fi':\n",
    "            sc = spacy.load('xx_sent_ud_sm')\n",
    "            for doc in tqdm(sc.pipe(corpus, disable=[\"lemmatizer\", \"textcat\", \"custom\"])): \n",
    "                doc_list = [word.text for word in doc]\n",
    "                \n",
    "                if lowercase:\n",
    "                    doc_list = [word.lower() for word in doc_list]\n",
    "\n",
    "                tokenized_corpus.append(doc_list)\n",
    "                \n",
    "            \n",
    "        \n",
    "#    if tool == 'NLTK':\n",
    "        \n",
    "        \n",
    "    return tokenized_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13496it [00:07, 1813.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# all as default ( with punctuation numbers, allll!)\n",
    "tokenized_fi1 = tokenization(corpus_fi, tool = 'spacy', language = 'fi',lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_regex(tokenized_corpus, letters = True, letters_and_numbers = False):\n",
    "    \n",
    "    if letters:\n",
    "        regex = r'[a-z]+'\n",
    "    if letters_and_numbers:\n",
    "        regex = r'([a-z]+|^\\d+$)'\n",
    "        \n",
    "    new_tokenized_zh = []\n",
    "    for sentence_list in tokenized_corpus:\n",
    "        sentence_list2 = [word for word in sentence_list if re.search(regex, word)]\n",
    "        new_tokenized_zh.append(sentence_list2)\n",
    "        \n",
    "    return new_tokenized_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation keeping the strings with letters\n",
    "tokenized_fi2 = match_regex(tokenized_fi1, letters = True, letters_and_numbers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation keeping the strings with letters or only numbers\n",
    "tokenized_fi3 = match_regex(tokenized_fi1, letters = False, letters_and_numbers = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172345it [13:43, 209.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# all as default\n",
    "spacy_tokenized_en1 = tokenization(corpus_en, tool = 'spacy', language = 'en', pos_to_remove = ['PUNCT','NUM'], ent_to_remove = ['PERSON','ORG'], stop_words_to_remove= False, lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172345it [14:14, 201.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# not removing entities names\n",
    "spacy_tokenized_en2 = tokenization(corpus_en, tool = 'spacy', language = 'en', pos_to_remove = ['PUNCT','NUM'], ent_to_remove = [], stop_words_to_remove= False, lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172345it [13:17, 215.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# not removing entities names and numbers\n",
    "spacy_tokenized_en3 = tokenization(corpus_en, tool = 'spacy', language = 'en', pos_to_remove = ['PUNCT'], ent_to_remove = [], stop_words_to_remove= False, lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['been',\n",
       " 'amount',\n",
       " 'quite',\n",
       " 'whereby',\n",
       " 'really',\n",
       " 'became',\n",
       " 'besides',\n",
       " 'no',\n",
       " 'whereupon',\n",
       " 'if',\n",
       " 'seeming',\n",
       " 'see',\n",
       " 'it',\n",
       " 'itself',\n",
       " 'serious',\n",
       " 'every',\n",
       " 'too',\n",
       " 'back',\n",
       " 'then',\n",
       " 'very']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords) #326\n",
    "list(spacy_stopwords)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jieba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\berfi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.853 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "jieba_tokenized_zh = []\n",
    "for sentence in corpus_zh:\n",
    "    sentence_list = [word for word in jieba.cut(sentence)]\n",
    "    jieba_tokenized_zh.append(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_regex(tokenized_corpus, chinese = True, chinese_and_numbers = False):\n",
    "    \n",
    "    if chinese:\n",
    "        regex = r'[\\u4e00-\\u9fff]+'\n",
    "    if chinese_and_numbers:\n",
    "        regex = r'([\\u4e00-\\u9fff]+|^\\d+$)'\n",
    "        \n",
    "    new_tokenized_zh = []\n",
    "    for sentence_list in tokenized_corpus:\n",
    "        chinese = [word for word in sentence_list if re.search(regex, word)]\n",
    "        new_tokenized_zh.append(chinese)\n",
    "        \n",
    "    return new_tokenized_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep all words containing chinese characters\n",
    "jieba_tokenized_zh2 = match_regex(jieba_tokenized_zh, chinese = True, chinese_and_numbers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep chinese characters and numbers\n",
    "jieba_tokenized_zh3 = match_regex(jieba_tokenized_zh, chinese = False, chinese_and_numbers = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46861it [04:41, 166.70it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_tokenized_zh = tokenization(corpus_zh, tool = 'spacy', language = 'zh', stop_words_to_remove= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['给',\n",
       " '何',\n",
       " '好的',\n",
       " '另',\n",
       " '逐渐',\n",
       " '哪怕',\n",
       " '是以',\n",
       " '［⑤ｂ］',\n",
       " 'ｆ］',\n",
       " '保持',\n",
       " '使得',\n",
       " '一样',\n",
       " '这般',\n",
       " '该',\n",
       " '［③ｄ］',\n",
       " '∪φ∈',\n",
       " '促进',\n",
       " '串行',\n",
       " '^',\n",
       " '能够']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.zh.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords) #1891\n",
    "list(spacy_stopwords)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep all words containing chinese characters\n",
    "spacy_tokenized_zh2 = match_regex(spacy_tokenized_zh, chinese = True, chinese_and_numbers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berfi\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pompeo', 0.7294467687606812),\n",
       " ('calvert', 0.7098347544670105),\n",
       " ('kansas', 0.7062785029411316),\n",
       " ('ken', 0.659417986869812),\n",
       " ('chairmen', 0.6563398838043213),\n",
       " ('mike', 0.6498099565505981),\n",
       " ('nominee', 0.645456075668335),\n",
       " ('democratic', 0.6383917331695557),\n",
       " ('candidate', 0.6352219581604004),\n",
       " ('vocal', 0.6339017748832703)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en = Word2Vec(sentences=spacy_tokenized_en3, min_count=1)\n",
    "model_en.save(\"word2vec_en.model\")\n",
    "\n",
    "model_en = Word2Vec.load(\"word2vec_en.model\")\n",
    "model_en.train(spacy_tokenized_en3, total_examples=1, epochs=1)\n",
    "\n",
    "vector_en = model_en.wv['republican']  # get numpy vector of a word\n",
    "sims_en = model_en.wv.most_similar('republican', topn=10)  # get other similar words\n",
    "sims_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('löydä', 0.9499693512916565),\n",
       " ('marcus', 0.9456921219825745),\n",
       " ('jatkoi', 0.9450461864471436),\n",
       " ('lanzmann', 0.94491046667099),\n",
       " ('käyttäjää', 0.9445856809616089),\n",
       " ('nations', 0.9436014890670776),\n",
       " ('eräänä', 0.9434420466423035),\n",
       " ('suositellaan', 0.9433552026748657),\n",
       " ('syyskuun', 0.9432809948921204),\n",
       " ('kommunistijohtaja', 0.9432252645492554)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fi = Word2Vec(sentences=tokenized_fi3, min_count=1)\n",
    "model_fi.save(\"word2vec_fi.model\")\n",
    "\n",
    "model_fi = Word2Vec.load(\"word2vec_fi.model\")\n",
    "model_fi.train(tokenized_fi3, total_examples=1, epochs=1)\n",
    "\n",
    "vector_fi = model_fi.wv['investoimaan']  # get numpy vector of a word\n",
    "sims_fi = model_fi.wv.most_similar('investoimaan', topn=10)  # get other similar words\n",
    "sims_fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('中华', 0.6370081901550293),\n",
       " ('义诊', 0.6322122812271118),\n",
       " ('交流年', 0.627692699432373),\n",
       " ('多元性', 0.6226692795753479),\n",
       " ('内涵', 0.6203840970993042),\n",
       " ('传播者', 0.6175974607467651),\n",
       " ('遗产', 0.6144717335700989),\n",
       " ('重任', 0.6135461330413818),\n",
       " ('跟岗', 0.6088530421257019),\n",
       " ('文化', 0.5952016115188599)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_zh = Word2Vec(sentences=spacy_tokenized_zh3, min_count=1)\n",
    "model_zh.save(\"word2vec_zh.model\")\n",
    "\n",
    "model_zh = Word2Vec.load(\"word2vec_zh.model\")\n",
    "model_zh.train(spacy_tokenized_zh3, total_examples=1, epochs=1)\n",
    "\n",
    "vector_zh = model_zh.wv['中国']  # get numpy vector of a word\n",
    "sims_zh = model_zh.wv.most_similar('中国', topn=10)  # get other similar words\n",
    "sims_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}