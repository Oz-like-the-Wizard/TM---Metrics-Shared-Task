{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd092ba50c92c9dc11b366869717e90d544d23b7140e20708921d0ff91f276d2e3f",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\oguzk\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_pairs = ['de-en', 'ru-en', 'zh-en']\n",
    "\n",
    "fi_df = pd.read_csv(\"corpus\\en-fi\\scores.csv\")\n",
    "zh_df = pd.read_csv(\"corpus\\en-zh\\scores.csv\")\n",
    "en_df = pd.read_csv(\"corpus\\cs-en\\scores.csv\")\n",
    "\n",
    "for pair in en_pairs:\n",
    "    new_df = pd.read_csv(os.path.join('corpus',pair, 'scores.csv'))\n",
    "    en_df = en_df.append(new_df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                  source  \\\n",
       "20000  Nach dem Brexit-Referendum ist es ungewiss, ob...   \n",
       "20001  Das Rudern an sich ist schon eine Herausforder...   \n",
       "20002  Dettori nimmt den Jubel auf, als er auf Predil...   \n",
       "20003  Durch die niedrigen Temperaturen läuft häufig ...   \n",
       "20004  Ihren Namen haben sie, weil sie scheinbar aus ...   \n",
       "\n",
       "                                               reference  \\\n",
       "20000  After the Brexit referendum, it is uncertain w...   \n",
       "20001  Rurowing in itself is already a challenge, but...   \n",
       "20002  Dettori takes the jubilee as he picks up victo...   \n",
       "20003  Due to low temperatures, the organism of the a...   \n",
       "20004  They have their name because they seem to come...   \n",
       "\n",
       "                                             translation   z-score  avg-score  \\\n",
       "20000  After the Brexit referendum it is unclear whet...  0.108148       96.0   \n",
       "20001  Rowing in itself is a journey but that makes i...  0.332057       98.0   \n",
       "20002  Dettori soaks up the cheers as he brings in Pr... -0.003806       95.0   \n",
       "20003  As a result of low temperatures, the systems o...  0.332057       98.0   \n",
       "20004  Their name refers to the fact that they seem t...  0.220102       97.0   \n",
       "\n",
       "       annotators  \n",
       "20000           2  \n",
       "20001           1  \n",
       "20002           1  \n",
       "20003           1  \n",
       "20004           1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>z-score</th>\n      <th>avg-score</th>\n      <th>annotators</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20000</th>\n      <td>Nach dem Brexit-Referendum ist es ungewiss, ob...</td>\n      <td>After the Brexit referendum, it is uncertain w...</td>\n      <td>After the Brexit referendum it is unclear whet...</td>\n      <td>0.108148</td>\n      <td>96.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>20001</th>\n      <td>Das Rudern an sich ist schon eine Herausforder...</td>\n      <td>Rurowing in itself is already a challenge, but...</td>\n      <td>Rowing in itself is a journey but that makes i...</td>\n      <td>0.332057</td>\n      <td>98.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20002</th>\n      <td>Dettori nimmt den Jubel auf, als er auf Predil...</td>\n      <td>Dettori takes the jubilee as he picks up victo...</td>\n      <td>Dettori soaks up the cheers as he brings in Pr...</td>\n      <td>-0.003806</td>\n      <td>95.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20003</th>\n      <td>Durch die niedrigen Temperaturen läuft häufig ...</td>\n      <td>Due to low temperatures, the organism of the a...</td>\n      <td>As a result of low temperatures, the systems o...</td>\n      <td>0.332057</td>\n      <td>98.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20004</th>\n      <td>Ihren Namen haben sie, weil sie scheinbar aus ...</td>\n      <td>They have their name because they seem to come...</td>\n      <td>Their name refers to the fact that they seem t...</td>\n      <td>0.220102</td>\n      <td>97.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "en_df.iloc[20000:20005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "77688it [02:51, 453.46it/s]\n",
      "77688it [02:50, 455.06it/s]\n"
     ]
    }
   ],
   "source": [
    "en_df = the_preprocessor(en_df, lang=\"en\", pos_list=['PUNCT','NUM'], ent_list=['PERSON','ORG'], stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              source   z-score  avg-score  \\\n",
       "0  Uchopíte pak zbraň mezi své předloktí a rameno... -0.675383  60.000000   \n",
       "1  Ale je-li New York změna, pak je to také znovu... -0.829403  44.000000   \n",
       "2  Dlouho a intenzivně jsem během léta přemýšlel,...  0.803185  96.500000   \n",
       "3         Najdou si jiný způsob, jak někde podvádět.  0.563149  90.500000   \n",
       "4  Zpráva o výměně v čele prezidentovy administra...  0.021549  74.666667   \n",
       "\n",
       "                                                  pt  \\\n",
       "0  [grasp, gun, forearm, shoulder, hitting, face,...   \n",
       "1                   [new, york, change, reinvention]   \n",
       "2  [thought, long, hard, course, summer, improve,...   \n",
       "3                               [find, way, defraud]   \n",
       "4  [news, replacement, president, office, come, s...   \n",
       "\n",
       "                                                 ref  1gram-precision  \\\n",
       "0  [grab, weapon, forearm, shoulder, hit, face, f...         0.625000   \n",
       "1                  [new, york, changed, rediscovery]         0.500000   \n",
       "2  [thinking, summer, improve, depth, needs, high...         0.500000   \n",
       "3                                 [find, way, cheat]         0.666667   \n",
       "4  [report, replacement, president, administratio...         0.333333   \n",
       "\n",
       "   2gram-precision  3gram-precision  4gram-precision  1gram-recall  \\\n",
       "0         0.428571         0.166667              0.0      0.625000   \n",
       "1         0.333333         0.000000              0.0      0.500000   \n",
       "2         0.272727         0.000000              0.0      0.857143   \n",
       "3         0.500000         0.000000              0.0      0.666667   \n",
       "4         0.125000         0.000000              0.0      0.300000   \n",
       "\n",
       "   2gram-recall  3gram-recall  4gram-recall    wmdist  pt_len  ref_len  \\\n",
       "0      0.428571      0.166667           0.0  0.376541       8        8   \n",
       "1      0.333333      0.000000           0.0  0.600992       4        4   \n",
       "2      0.500000      0.000000           0.0  0.616613      12        7   \n",
       "3      0.500000      0.000000           0.0  0.203281       3        3   \n",
       "4      0.111111      0.000000           0.0  0.810394       9       10   \n",
       "\n",
       "   len_diff  \n",
       "0         0  \n",
       "1         0  \n",
       "2         5  \n",
       "3         0  \n",
       "4        -1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>z-score</th>\n      <th>avg-score</th>\n      <th>pt</th>\n      <th>ref</th>\n      <th>1gram-precision</th>\n      <th>2gram-precision</th>\n      <th>3gram-precision</th>\n      <th>4gram-precision</th>\n      <th>1gram-recall</th>\n      <th>2gram-recall</th>\n      <th>3gram-recall</th>\n      <th>4gram-recall</th>\n      <th>wmdist</th>\n      <th>pt_len</th>\n      <th>ref_len</th>\n      <th>len_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Uchopíte pak zbraň mezi své předloktí a rameno...</td>\n      <td>-0.675383</td>\n      <td>60.000000</td>\n      <td>[grasp, gun, forearm, shoulder, hitting, face,...</td>\n      <td>[grab, weapon, forearm, shoulder, hit, face, f...</td>\n      <td>0.625000</td>\n      <td>0.428571</td>\n      <td>0.166667</td>\n      <td>0.0</td>\n      <td>0.625000</td>\n      <td>0.428571</td>\n      <td>0.166667</td>\n      <td>0.0</td>\n      <td>0.376541</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ale je-li New York změna, pak je to také znovu...</td>\n      <td>-0.829403</td>\n      <td>44.000000</td>\n      <td>[new, york, change, reinvention]</td>\n      <td>[new, york, changed, rediscovery]</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.600992</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dlouho a intenzivně jsem během léta přemýšlel,...</td>\n      <td>0.803185</td>\n      <td>96.500000</td>\n      <td>[thought, long, hard, course, summer, improve,...</td>\n      <td>[thinking, summer, improve, depth, needs, high...</td>\n      <td>0.500000</td>\n      <td>0.272727</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.857143</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.616613</td>\n      <td>12</td>\n      <td>7</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Najdou si jiný způsob, jak někde podvádět.</td>\n      <td>0.563149</td>\n      <td>90.500000</td>\n      <td>[find, way, defraud]</td>\n      <td>[find, way, cheat]</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.203281</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Zpráva o výměně v čele prezidentovy administra...</td>\n      <td>0.021549</td>\n      <td>74.666667</td>\n      <td>[news, replacement, president, office, come, s...</td>\n      <td>[report, replacement, president, administratio...</td>\n      <td>0.333333</td>\n      <td>0.125000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.300000</td>\n      <td>0.111111</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.810394</td>\n      <td>9</td>\n      <td>10</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "en_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              source   z-score  avg-score  \\\n",
       "0  Uchopíte pak zbraň mezi své předloktí a rameno... -0.675383  60.000000   \n",
       "1  Ale je-li New York změna, pak je to také znovu... -0.829403  44.000000   \n",
       "2  Dlouho a intenzivně jsem během léta přemýšlel,...  0.803185  96.500000   \n",
       "3         Najdou si jiný způsob, jak někde podvádět.  0.563149  90.500000   \n",
       "4  Zpráva o výměně v čele prezidentovy administra...  0.021549  74.666667   \n",
       "\n",
       "                                                  pt  \\\n",
       "0  [grasp, gun, forearm, shoulder, hitting, face,...   \n",
       "1                   [new, york, change, reinvention]   \n",
       "2  [thought, long, hard, course, summer, improve,...   \n",
       "3                               [find, way, defraud]   \n",
       "4  [news, replacement, president, office, come, s...   \n",
       "\n",
       "                                                 ref  1gram-precision  \\\n",
       "0  [grab, weapon, forearm, shoulder, hit, face, f...         0.625000   \n",
       "1                  [new, york, changed, rediscovery]         0.500000   \n",
       "2  [thinking, summer, improve, depth, needs, high...         0.500000   \n",
       "3                                 [find, way, cheat]         0.666667   \n",
       "4  [report, replacement, president, administratio...         0.333333   \n",
       "\n",
       "   2gram-precision  3gram-precision  4gram-precision  1gram-recall  \\\n",
       "0         0.428571         0.166667              0.0      0.625000   \n",
       "1         0.333333         0.000000              0.0      0.500000   \n",
       "2         0.272727         0.000000              0.0      0.857143   \n",
       "3         0.500000         0.000000              0.0      0.666667   \n",
       "4         0.125000         0.000000              0.0      0.300000   \n",
       "\n",
       "   2gram-recall  3gram-recall  4gram-recall    wmdist  pt_len  ref_len  \\\n",
       "0      0.428571      0.166667           0.0  0.329779       8        8   \n",
       "1      0.333333      0.000000           0.0  0.556544       4        4   \n",
       "2      0.500000      0.000000           0.0  0.560231      12        7   \n",
       "3      0.500000      0.000000           0.0  0.245984       3        3   \n",
       "4      0.111111      0.000000           0.0  0.819325       9       10   \n",
       "\n",
       "   len_diff  \n",
       "0         0  \n",
       "1         0  \n",
       "2         5  \n",
       "3         0  \n",
       "4        -1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>z-score</th>\n      <th>avg-score</th>\n      <th>pt</th>\n      <th>ref</th>\n      <th>1gram-precision</th>\n      <th>2gram-precision</th>\n      <th>3gram-precision</th>\n      <th>4gram-precision</th>\n      <th>1gram-recall</th>\n      <th>2gram-recall</th>\n      <th>3gram-recall</th>\n      <th>4gram-recall</th>\n      <th>wmdist</th>\n      <th>pt_len</th>\n      <th>ref_len</th>\n      <th>len_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Uchopíte pak zbraň mezi své předloktí a rameno...</td>\n      <td>-0.675383</td>\n      <td>60.000000</td>\n      <td>[grasp, gun, forearm, shoulder, hitting, face,...</td>\n      <td>[grab, weapon, forearm, shoulder, hit, face, f...</td>\n      <td>0.625000</td>\n      <td>0.428571</td>\n      <td>0.166667</td>\n      <td>0.0</td>\n      <td>0.625000</td>\n      <td>0.428571</td>\n      <td>0.166667</td>\n      <td>0.0</td>\n      <td>0.329779</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ale je-li New York změna, pak je to také znovu...</td>\n      <td>-0.829403</td>\n      <td>44.000000</td>\n      <td>[new, york, change, reinvention]</td>\n      <td>[new, york, changed, rediscovery]</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.556544</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dlouho a intenzivně jsem během léta přemýšlel,...</td>\n      <td>0.803185</td>\n      <td>96.500000</td>\n      <td>[thought, long, hard, course, summer, improve,...</td>\n      <td>[thinking, summer, improve, depth, needs, high...</td>\n      <td>0.500000</td>\n      <td>0.272727</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.857143</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.560231</td>\n      <td>12</td>\n      <td>7</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Najdou si jiný způsob, jak někde podvádět.</td>\n      <td>0.563149</td>\n      <td>90.500000</td>\n      <td>[find, way, defraud]</td>\n      <td>[find, way, cheat]</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.245984</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Zpráva o výměně v čele prezidentovy administra...</td>\n      <td>0.021549</td>\n      <td>74.666667</td>\n      <td>[news, replacement, president, office, come, s...</td>\n      <td>[report, replacement, president, administratio...</td>\n      <td>0.333333</td>\n      <td>0.125000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.300000</td>\n      <td>0.111111</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.819325</td>\n      <td>9</td>\n      <td>10</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "en_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "6748it [00:01, 6464.51it/s]\n",
      "6748it [00:01, 4841.65it/s]\n"
     ]
    }
   ],
   "source": [
    "fi_df = the_preprocessor(fi_df, lang=\"fi\", lowercase=True, let=True, letandnum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              source   z-score  avg-score  \\\n",
       "0  You can turn yourself into a pineapple, a dog ... -0.286195      34.20   \n",
       "1  Also shot were three men: two 29-year-olds and...  0.547076      58.40   \n",
       "2  The information is stored at the cash register...  1.122476      74.60   \n",
       "3  Xinhua says that there were traces of hydrochl...  0.383095      53.60   \n",
       "4  MacDonald, who was brought on board CBC's comm... -0.493065      32.25   \n",
       "\n",
       "                                                  pt  \\\n",
       "0  [voit, muuttaa, itsesi, ananakseksi, koiraksi,...   \n",
       "1  [myös, kolmea, miestä, ammuttiin, kahta, 29-vu...   \n",
       "2  [tiedot, kuitenkin, tallentuvat, kassoilla, jo...   \n",
       "3  [xinhua, kertoo, että, xinyin, sunnuntaina, an...   \n",
       "4  [macdonaldin, joka, tuli, cbc, n, selostajatii...   \n",
       "\n",
       "                                                 ref  1gram-precision  \\\n",
       "0  [voit, muuttaa, itsesi, ananasta, koirasta, ta...         0.500000   \n",
       "1  [myös, ammuttiin, kolme, miestä, kaksi, 29-vuo...         0.444444   \n",
       "2  [tiedot, tallennetaan, kassakoneisiin, joka, t...         0.400000   \n",
       "3  [xinhua, kertoo, että, xinyin, näytteestä, oli...         0.461538   \n",
       "4  [voitaisiin, kuulla, cbd, n, kommenttitiimin, ...         0.250000   \n",
       "\n",
       "   2gram-precision  3gram-precision  4gram-precision  1gram-recall  \\\n",
       "0         0.272727         0.100000              0.0      0.545455   \n",
       "1         0.000000         0.000000              0.0      0.444444   \n",
       "2         0.111111         0.000000              0.0      0.444444   \n",
       "3         0.250000         0.181818              0.1      0.750000   \n",
       "4         0.066667         0.000000              0.0      0.250000   \n",
       "\n",
       "   2gram-recall  3gram-recall  4gram-recall    wmdist  pt_len  ref_len  \\\n",
       "0      0.300000      0.111111           0.0  0.367791      12       11   \n",
       "1      0.000000      0.000000           0.0  0.307341       9        9   \n",
       "2      0.125000      0.000000           0.0  0.292713      10        9   \n",
       "3      0.428571      0.333333           0.2  0.238081      13        8   \n",
       "4      0.066667      0.000000           0.0  0.221037      16       16   \n",
       "\n",
       "   len_diff  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         5  \n",
       "4         0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>z-score</th>\n      <th>avg-score</th>\n      <th>pt</th>\n      <th>ref</th>\n      <th>1gram-precision</th>\n      <th>2gram-precision</th>\n      <th>3gram-precision</th>\n      <th>4gram-precision</th>\n      <th>1gram-recall</th>\n      <th>2gram-recall</th>\n      <th>3gram-recall</th>\n      <th>4gram-recall</th>\n      <th>wmdist</th>\n      <th>pt_len</th>\n      <th>ref_len</th>\n      <th>len_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>You can turn yourself into a pineapple, a dog ...</td>\n      <td>-0.286195</td>\n      <td>34.20</td>\n      <td>[voit, muuttaa, itsesi, ananakseksi, koiraksi,...</td>\n      <td>[voit, muuttaa, itsesi, ananasta, koirasta, ta...</td>\n      <td>0.500000</td>\n      <td>0.272727</td>\n      <td>0.100000</td>\n      <td>0.0</td>\n      <td>0.545455</td>\n      <td>0.300000</td>\n      <td>0.111111</td>\n      <td>0.0</td>\n      <td>0.367791</td>\n      <td>12</td>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Also shot were three men: two 29-year-olds and...</td>\n      <td>0.547076</td>\n      <td>58.40</td>\n      <td>[myös, kolmea, miestä, ammuttiin, kahta, 29-vu...</td>\n      <td>[myös, ammuttiin, kolme, miestä, kaksi, 29-vuo...</td>\n      <td>0.444444</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.444444</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.307341</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The information is stored at the cash register...</td>\n      <td>1.122476</td>\n      <td>74.60</td>\n      <td>[tiedot, kuitenkin, tallentuvat, kassoilla, jo...</td>\n      <td>[tiedot, tallennetaan, kassakoneisiin, joka, t...</td>\n      <td>0.400000</td>\n      <td>0.111111</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.444444</td>\n      <td>0.125000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.292713</td>\n      <td>10</td>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Xinhua says that there were traces of hydrochl...</td>\n      <td>0.383095</td>\n      <td>53.60</td>\n      <td>[xinhua, kertoo, että, xinyin, sunnuntaina, an...</td>\n      <td>[xinhua, kertoo, että, xinyin, näytteestä, oli...</td>\n      <td>0.461538</td>\n      <td>0.250000</td>\n      <td>0.181818</td>\n      <td>0.1</td>\n      <td>0.750000</td>\n      <td>0.428571</td>\n      <td>0.333333</td>\n      <td>0.2</td>\n      <td>0.238081</td>\n      <td>13</td>\n      <td>8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MacDonald, who was brought on board CBC's comm...</td>\n      <td>-0.493065</td>\n      <td>32.25</td>\n      <td>[macdonaldin, joka, tuli, cbc, n, selostajatii...</td>\n      <td>[voitaisiin, kuulla, cbd, n, kommenttitiimin, ...</td>\n      <td>0.250000</td>\n      <td>0.066667</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.066667</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.221037</td>\n      <td>16</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "fi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "10221it [00:27, 370.23it/s]\n",
      "10221it [00:28, 361.74it/s]\n"
     ]
    }
   ],
   "source": [
    "zh_df = the_preprocessor(zh_df, \"zh\", stopwords=True, let=True, letandnum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              source   z-score  avg-score  \\\n",
       "0  \"In the GISS model's simulation, Venus' slow s... -1.171867       50.0   \n",
       "1  Ai Yanhan of China in the Women's 4 x 200m Fre... -2.255403       26.5   \n",
       "2  Then came 2012, when nothing much went right f... -2.508996       21.0   \n",
       "3  Since last year, Guodian Group has exported a ... -2.416780       23.0   \n",
       "4  Some alleged that the Kempinski hotel simply \"... -1.489676       45.0   \n",
       "\n",
       "                                                  pt  \\\n",
       "0  [戈达德, 太空, 研究所, 科学家, 安东尼·德尔·杰尼奥, 新闻, 发布会, 解释, 说...   \n",
       "1  [参加, 女子, 米, 自由泳, 接力赛, 决赛, 中国, 小将, 艾衍, 含, 描述, 名...   \n",
       "2                                    [2012年, 队友, 看好]   \n",
       "3    [去年, 国电, 集团, 共计, 套, 风电, 项目, 陆续, 连云港, 港, 出口, 南非]   \n",
       "4                      [有人, 凯宾斯基, 酒店, 阿拉伯, 客户, 卑躬屈膝]   \n",
       "\n",
       "                                                 ref  1gram-precision  \\\n",
       "0  [科学家, 新闻稿, 中, 解释, 说, S模型, 模拟, 模型, 中, 金星, 缓慢, 太...         0.400000   \n",
       "1         [中国, 英国, 女性, 中, 称为, 中国, 岁, 孩子, 球下, 降到, 孩子]         0.166667   \n",
       "2                               [来到, 2012年, 队友们, 好处]         0.333333   \n",
       "3                [去年, 南非, 南非, 港口, 出口, 套, 风力, 发电, 项目]         0.416667   \n",
       "4                              [指称, 旅馆, 捕\", 阿拉伯, 客户]         0.333333   \n",
       "\n",
       "   2gram-precision  3gram-precision  4gram-precision  1gram-recall  \\\n",
       "0         0.166667         0.043478              0.0      0.714286   \n",
       "1         0.000000         0.000000              0.0      0.272727   \n",
       "2         0.000000         0.000000              0.0      0.250000   \n",
       "3         0.000000         0.000000              0.0      0.555556   \n",
       "4         0.200000         0.000000              0.0      0.400000   \n",
       "\n",
       "   2gram-recall  3gram-recall  4gram-recall    wmdist  pt_len  ref_len  \\\n",
       "0      0.307692      0.083333           0.0  0.482995      25       14   \n",
       "1      0.000000      0.000000           0.0  0.755570      18       11   \n",
       "2      0.000000      0.000000           0.0  0.766106       3        4   \n",
       "3      0.000000      0.000000           0.0  0.399447      12        9   \n",
       "4      0.250000      0.000000           0.0  0.542373       6        5   \n",
       "\n",
       "   len_diff  \n",
       "0        11  \n",
       "1         7  \n",
       "2        -1  \n",
       "3         3  \n",
       "4         1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>z-score</th>\n      <th>avg-score</th>\n      <th>pt</th>\n      <th>ref</th>\n      <th>1gram-precision</th>\n      <th>2gram-precision</th>\n      <th>3gram-precision</th>\n      <th>4gram-precision</th>\n      <th>1gram-recall</th>\n      <th>2gram-recall</th>\n      <th>3gram-recall</th>\n      <th>4gram-recall</th>\n      <th>wmdist</th>\n      <th>pt_len</th>\n      <th>ref_len</th>\n      <th>len_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"In the GISS model's simulation, Venus' slow s...</td>\n      <td>-1.171867</td>\n      <td>50.0</td>\n      <td>[戈达德, 太空, 研究所, 科学家, 安东尼·德尔·杰尼奥, 新闻, 发布会, 解释, 说...</td>\n      <td>[科学家, 新闻稿, 中, 解释, 说, S模型, 模拟, 模型, 中, 金星, 缓慢, 太...</td>\n      <td>0.400000</td>\n      <td>0.166667</td>\n      <td>0.043478</td>\n      <td>0.0</td>\n      <td>0.714286</td>\n      <td>0.307692</td>\n      <td>0.083333</td>\n      <td>0.0</td>\n      <td>0.482995</td>\n      <td>25</td>\n      <td>14</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ai Yanhan of China in the Women's 4 x 200m Fre...</td>\n      <td>-2.255403</td>\n      <td>26.5</td>\n      <td>[参加, 女子, 米, 自由泳, 接力赛, 决赛, 中国, 小将, 艾衍, 含, 描述, 名...</td>\n      <td>[中国, 英国, 女性, 中, 称为, 中国, 岁, 孩子, 球下, 降到, 孩子]</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.272727</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.755570</td>\n      <td>18</td>\n      <td>11</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Then came 2012, when nothing much went right f...</td>\n      <td>-2.508996</td>\n      <td>21.0</td>\n      <td>[2012年, 队友, 看好]</td>\n      <td>[来到, 2012年, 队友们, 好处]</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.766106</td>\n      <td>3</td>\n      <td>4</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Since last year, Guodian Group has exported a ...</td>\n      <td>-2.416780</td>\n      <td>23.0</td>\n      <td>[去年, 国电, 集团, 共计, 套, 风电, 项目, 陆续, 连云港, 港, 出口, 南非]</td>\n      <td>[去年, 南非, 南非, 港口, 出口, 套, 风力, 发电, 项目]</td>\n      <td>0.416667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.555556</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.399447</td>\n      <td>12</td>\n      <td>9</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Some alleged that the Kempinski hotel simply \"...</td>\n      <td>-1.489676</td>\n      <td>45.0</td>\n      <td>[有人, 凯宾斯基, 酒店, 阿拉伯, 客户, 卑躬屈膝]</td>\n      <td>[指称, 旅馆, 捕\", 阿拉伯, 客户]</td>\n      <td>0.333333</td>\n      <td>0.200000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.400000</td>\n      <td>0.250000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.542373</td>\n      <td>6</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "zh_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = en_df.append(fi_df, ignore_index=True) \n",
    "total_df = total_df.append(zh_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final inf & NaN Check\n",
    " \n",
    "total_df = total_df.replace(np.inf, np.nan)\n",
    "total_df = total_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(77688, 17)\n(6748, 17)\n(10221, 17)\n"
     ]
    }
   ],
   "source": [
    "print(en_df.shape)\n",
    "print(fi_df.shape)\n",
    "print(zh_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df = en_df.replace(np.inf, np.nan)\n",
    "en_df = en_df.dropna()\n",
    "\n",
    " \n",
    "fi_df = fi_df.replace(np.inf, np.nan)\n",
    "fi_df = fi_df.dropna()\n",
    " \n",
    "zh_df = zh_df.replace(np.inf, np.nan)\n",
    "zh_df = zh_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(77487, 17)\n(6748, 17)\n(10193, 17)\n"
     ]
    }
   ],
   "source": [
    "print(en_df.shape)\n",
    "print(fi_df.shape)\n",
    "print(zh_df.shape)"
   ]
  },
  {
   "source": [
    "## Neural Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              source   z-score  avg-score  \\\n",
       "0  Uchopíte pak zbraň mezi své předloktí a rameno... -0.675383  60.000000   \n",
       "1  Ale je-li New York změna, pak je to také znovu... -0.829403  44.000000   \n",
       "2  Dlouho a intenzivně jsem během léta přemýšlel,...  0.803185  96.500000   \n",
       "3         Najdou si jiný způsob, jak někde podvádět.  0.563149  90.500000   \n",
       "4  Zpráva o výměně v čele prezidentovy administra...  0.021549  74.666667   \n",
       "\n",
       "                                                  pt  \\\n",
       "0  [grasp, gun, forearm, shoulder, hitting, face,...   \n",
       "1                   [new, york, change, reinvention]   \n",
       "2  [thought, long, hard, course, summer, improve,...   \n",
       "3                               [find, way, defraud]   \n",
       "4  [news, replacement, president, office, come, s...   \n",
       "\n",
       "                                                 ref  1gram-precision  \\\n",
       "0  [grab, weapon, forearm, shoulder, hit, face, f...         0.625000   \n",
       "1                  [new, york, changed, rediscovery]         0.500000   \n",
       "2  [thinking, summer, improve, depth, needs, high...         0.500000   \n",
       "3                                 [find, way, cheat]         0.666667   \n",
       "4  [report, replacement, president, administratio...         0.333333   \n",
       "\n",
       "   2gram-precision  3gram-precision  4gram-precision  1gram-recall  \\\n",
       "0         0.428571         0.166667              0.0      0.625000   \n",
       "1         0.333333         0.000000              0.0      0.500000   \n",
       "2         0.272727         0.000000              0.0      0.857143   \n",
       "3         0.500000         0.000000              0.0      0.666667   \n",
       "4         0.125000         0.000000              0.0      0.300000   \n",
       "\n",
       "   2gram-recall  3gram-recall  4gram-recall    wmdist  pt_len  ref_len  \\\n",
       "0      0.428571      0.166667           0.0  0.329779       8        8   \n",
       "1      0.333333      0.000000           0.0  0.556544       4        4   \n",
       "2      0.500000      0.000000           0.0  0.560231      12        7   \n",
       "3      0.500000      0.000000           0.0  0.245984       3        3   \n",
       "4      0.111111      0.000000           0.0  0.819325       9       10   \n",
       "\n",
       "   len_diff  \n",
       "0         0  \n",
       "1         0  \n",
       "2         5  \n",
       "3         0  \n",
       "4        -1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>z-score</th>\n      <th>avg-score</th>\n      <th>pt</th>\n      <th>ref</th>\n      <th>1gram-precision</th>\n      <th>2gram-precision</th>\n      <th>3gram-precision</th>\n      <th>4gram-precision</th>\n      <th>1gram-recall</th>\n      <th>2gram-recall</th>\n      <th>3gram-recall</th>\n      <th>4gram-recall</th>\n      <th>wmdist</th>\n      <th>pt_len</th>\n      <th>ref_len</th>\n      <th>len_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Uchopíte pak zbraň mezi své předloktí a rameno...</td>\n      <td>-0.675383</td>\n      <td>60.000000</td>\n      <td>[grasp, gun, forearm, shoulder, hitting, face,...</td>\n      <td>[grab, weapon, forearm, shoulder, hit, face, f...</td>\n      <td>0.625000</td>\n      <td>0.428571</td>\n      <td>0.166667</td>\n      <td>0.0</td>\n      <td>0.625000</td>\n      <td>0.428571</td>\n      <td>0.166667</td>\n      <td>0.0</td>\n      <td>0.329779</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ale je-li New York změna, pak je to také znovu...</td>\n      <td>-0.829403</td>\n      <td>44.000000</td>\n      <td>[new, york, change, reinvention]</td>\n      <td>[new, york, changed, rediscovery]</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.556544</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dlouho a intenzivně jsem během léta přemýšlel,...</td>\n      <td>0.803185</td>\n      <td>96.500000</td>\n      <td>[thought, long, hard, course, summer, improve,...</td>\n      <td>[thinking, summer, improve, depth, needs, high...</td>\n      <td>0.500000</td>\n      <td>0.272727</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.857143</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.560231</td>\n      <td>12</td>\n      <td>7</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Najdou si jiný způsob, jak někde podvádět.</td>\n      <td>0.563149</td>\n      <td>90.500000</td>\n      <td>[find, way, defraud]</td>\n      <td>[find, way, cheat]</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.245984</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Zpráva o výměně v čele prezidentovy administra...</td>\n      <td>0.021549</td>\n      <td>74.666667</td>\n      <td>[news, replacement, president, office, come, s...</td>\n      <td>[report, replacement, president, administratio...</td>\n      <td>0.333333</td>\n      <td>0.125000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.300000</td>\n      <td>0.111111</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.819325</td>\n      <td>9</td>\n      <td>10</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "en_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data ready for NN \n",
    "\n",
    "# data, target\n",
    "target = en_df[\"avg-score\"]\n",
    "X = en_df.drop([\"source\",\"avg-score\",\"z-score\",\"pt\",\"ref\",\"pt_len\",\"ref_len\"], axis=1)\n",
    "\n",
    "# standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sscaler = StandardScaler()\n",
    "X_ss = sscaler.fit_transform(X)\n",
    "\n",
    "# train-test-split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.20, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data ready for NN \n",
    "long_en_df = en_df[(en_df[\"pt_len\"]>3) & (en_df[\"ref_len\"]>3)]\n",
    "\n",
    "# data, target\n",
    "long_target = en_df[\"avg-score\"]\n",
    "long_X = long_en_df.drop([\"source\",\"avg-score\",\"z-score\",\"pt\",\"ref\",\"pt_len\",\"ref_len\"], axis=1)\n",
    "\n",
    "# standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sscaler = StandardScaler()\n",
    "long_X_ss = sscaler.fit_transform(long_X)\n",
    "\n",
    "# train-test-split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.20, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data ready for NN \n",
    "long3_en_df = en_df[(en_df[\"pt_len\"]>3) & (en_df[\"ref_len\"]>3)]\n",
    "\n",
    "# data, target\n",
    "long3_target = en_df[\"avg-score\"]\n",
    "long3_X = long3_en_df.drop([\"source\",\"avg-score\",\"z-score\",\"pt\",\"ref\",\"pt_len\",\"ref_len\",'4gram-recall', '4gram-precision'], axis=1)\n",
    "\n",
    "# standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sscaler = StandardScaler()\n",
    "long3_X_ss = sscaler.fit_transform(long3_X)\n",
    "\n",
    "# train-test-split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.20, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data ready for NN \n",
    "long_en_df = en_df[(en_df[\"pt_len\"]>3) & (en_df[\"ref_len\"]>3)]\n",
    "\n",
    "# data, target\n",
    "longp_target = en_df[\"avg-score\"]\n",
    "longp_X = long3_en_df.drop([\"source\",\"avg-score\",\"z-score\",\"pt\",\"ref\",\"pt_len\",\"ref_len\",'4gram-recall', '3gram-recall','2gram-recall','1gram-recall'], axis=1)\n",
    "\n",
    "# standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sscaler = StandardScaler()\n",
    "longp_X_ss = sscaler.fit_transform(longp_X)\n",
    "\n",
    "# train-test-split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.20, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data ready for NN \n",
    "long_en_df = en_df[(en_df[\"pt_len\"]>3) & (en_df[\"ref_len\"]>3)]\n",
    "\n",
    "# data, target\n",
    "longbasic_target = en_df[\"avg-score\"]\n",
    "longbasic_X = long_en_df.drop([\"source\",\"avg-score\",\"z-score\",\"pt\",\"ref\",\"pt_len\",\"ref_len\",'4gram-recall', '3gram-recall','2gram-recall','2gram-precision', '3gram-precision', '4gram-precision'], axis=1)\n",
    "\n",
    "# standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sscaler = StandardScaler()\n",
    "longbasic_X_ss = sscaler.fit_transform(longbasic_X)\n",
    "\n",
    "# train-test-split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.20, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data ready for NN \n",
    "long_en_df = en_df[(en_df[\"pt_len\"]>3) & (en_df[\"ref_len\"]>3)]\n",
    "\n",
    "# data, target\n",
    "longgram_target = en_df[\"avg-score\"]\n",
    "longgram_X = long_en_df.drop([\"source\",\"avg-score\",\"z-score\",\"pt\",\"ref\",\"pt_len\",\"ref_len\",'wmdist'], axis=1)\n",
    "\n",
    "# standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sscaler = StandardScaler()\n",
    "longgram_X_ss = sscaler.fit_transform(longgram_X)\n",
    "\n",
    "# train-test-split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.20, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['1gram-precision', '2gram-precision', '3gram-precision',\n",
       "       '4gram-precision', '1gram-recall', '2gram-recall', '3gram-recall',\n",
       "       '4gram-recall', 'len_diff'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "longgram_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_15\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_53 (Dense)             (None, 128)               1408      \n_________________________________________________________________\ndense_54 (Dense)             (None, 256)               33024     \n_________________________________________________________________\ndense_55 (Dense)             (None, 256)               65792     \n_________________________________________________________________\ndense_56 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndense_57 (Dense)             (None, 1)                 129       \n=================================================================\nTotal params: 133,249\nTrainable params: 133,249\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnn1 = [10,128,256, 256, 128,1]\n",
    "mm1 = create_model(nnn1)\n",
    "mm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 1513.7592 - mae: 32.4189 - val_loss: 804.2795 - val_mae: 22.5340\n",
      "Epoch 2/20\n",
      "1938/1938 [==============================] - 3s 1ms/step - loss: 676.8774 - mae: 21.1270 - val_loss: 815.7429 - val_mae: 22.5570\n",
      "Epoch 3/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 672.8691 - mae: 21.0613 - val_loss: 763.5386 - val_mae: 21.9275\n",
      "Epoch 4/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 635.0023 - mae: 20.2736 - val_loss: 753.4001 - val_mae: 21.7629\n",
      "Epoch 5/20\n",
      "1938/1938 [==============================] - 3s 1ms/step - loss: 629.4063 - mae: 20.1422 - val_loss: 735.2061 - val_mae: 21.6825\n",
      "Epoch 6/20\n",
      "1938/1938 [==============================] - 3s 1ms/step - loss: 627.2863 - mae: 20.0729 - val_loss: 757.8402 - val_mae: 21.6903\n",
      "Epoch 7/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 625.5745 - mae: 20.0455 - val_loss: 739.2214 - val_mae: 21.7404\n",
      "Epoch 8/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 624.8187 - mae: 20.0195 - val_loss: 736.7429 - val_mae: 21.6230\n",
      "Epoch 9/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 624.3436 - mae: 20.0096 - val_loss: 751.1385 - val_mae: 21.6205\n",
      "Epoch 10/20\n",
      "1938/1938 [==============================] - 3s 1ms/step - loss: 623.9117 - mae: 20.0068 - val_loss: 744.4251 - val_mae: 21.6241\n",
      "Epoch 11/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 623.6171 - mae: 19.9912 - val_loss: 760.7885 - val_mae: 21.6809\n",
      "Epoch 12/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 623.2277 - mae: 19.9683 - val_loss: 733.1317 - val_mae: 21.5948\n",
      "Epoch 13/20\n",
      "1938/1938 [==============================] - 3s 1ms/step - loss: 623.1212 - mae: 19.9734 - val_loss: 739.6809 - val_mae: 21.6585\n",
      "Epoch 14/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 622.6668 - mae: 19.9695 - val_loss: 747.9380 - val_mae: 21.6193\n",
      "Epoch 15/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 622.8111 - mae: 19.9561 - val_loss: 735.0415 - val_mae: 21.6009\n",
      "Epoch 16/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 622.6078 - mae: 19.9702 - val_loss: 749.7095 - val_mae: 21.5773\n",
      "Epoch 17/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 622.5759 - mae: 19.9585 - val_loss: 734.4196 - val_mae: 21.5803\n",
      "Epoch 18/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 622.3308 - mae: 19.9539 - val_loss: 737.2490 - val_mae: 21.5767\n",
      "Epoch 19/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 622.6693 - mae: 19.9575 - val_loss: 747.7913 - val_mae: 21.6340\n",
      "Epoch 20/20\n",
      "1938/1938 [==============================] - 3s 2ms/step - loss: 622.7376 - mae: 19.9383 - val_loss: 748.5951 - val_mae: 21.5882\n"
     ]
    }
   ],
   "source": [
    "hist1 = mm1.fit(X_ss, target, epochs=20, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_17 (Dense)             (None, 16)                144       \n_________________________________________________________________\ndense_18 (Dense)             (None, 32)                544       \n_________________________________________________________________\ndense_19 (Dense)             (None, 16)                528       \n_________________________________________________________________\ndense_20 (Dense)             (None, 1)                 17        \n=================================================================\nTotal params: 1,233\nTrainable params: 1,233\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nnn2 = [8, 16, 32, 16, 1]\n",
    "mm2 = create_model(nnn2)\n",
    "mm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1736/1736 [==============================] - 1s 673us/step - loss: 1157.1362 - mae: 26.5240 - val_loss: 733.9861 - val_mae: 21.6193\n",
      "Epoch 2/20\n",
      "1736/1736 [==============================] - 1s 630us/step - loss: 682.3199 - mae: 21.2003 - val_loss: 726.4310 - val_mae: 21.6623\n",
      "Epoch 3/20\n",
      "1736/1736 [==============================] - 1s 626us/step - loss: 681.2580 - mae: 21.1942 - val_loss: 763.2473 - val_mae: 21.4291\n",
      "Epoch 4/20\n",
      "1736/1736 [==============================] - 1s 693us/step - loss: 680.7361 - mae: 21.1654 - val_loss: 723.8104 - val_mae: 21.7240\n",
      "Epoch 5/20\n",
      "1736/1736 [==============================] - 1s 658us/step - loss: 681.1065 - mae: 21.1803 - val_loss: 743.5319 - val_mae: 21.5026\n",
      "Epoch 6/20\n",
      "1736/1736 [==============================] - 1s 630us/step - loss: 680.7977 - mae: 21.1755 - val_loss: 766.8736 - val_mae: 21.4606\n",
      "Epoch 7/20\n",
      "1736/1736 [==============================] - 1s 635us/step - loss: 680.8561 - mae: 21.1724 - val_loss: 746.9894 - val_mae: 21.4411\n",
      "Epoch 8/20\n",
      "1736/1736 [==============================] - 1s 650us/step - loss: 681.0941 - mae: 21.1770 - val_loss: 731.7411 - val_mae: 21.5668\n",
      "Epoch 9/20\n",
      "1736/1736 [==============================] - 1s 646us/step - loss: 681.1294 - mae: 21.1803 - val_loss: 741.4882 - val_mae: 21.4700\n",
      "Epoch 10/20\n",
      "1736/1736 [==============================] - 1s 633us/step - loss: 680.7604 - mae: 21.1728 - val_loss: 738.8300 - val_mae: 21.4836\n",
      "Epoch 11/20\n",
      "1736/1736 [==============================] - 1s 649us/step - loss: 680.8054 - mae: 21.1819 - val_loss: 733.2064 - val_mae: 21.5252\n",
      "Epoch 12/20\n",
      "1736/1736 [==============================] - 1s 639us/step - loss: 680.7162 - mae: 21.1746 - val_loss: 727.8452 - val_mae: 21.5558\n",
      "Epoch 13/20\n",
      "1736/1736 [==============================] - 1s 644us/step - loss: 680.7577 - mae: 21.1761 - val_loss: 722.7631 - val_mae: 21.6573\n",
      "Epoch 14/20\n",
      "1736/1736 [==============================] - 1s 643us/step - loss: 681.0292 - mae: 21.1831 - val_loss: 729.8445 - val_mae: 21.5775\n",
      "Epoch 15/20\n",
      "1736/1736 [==============================] - 1s 635us/step - loss: 680.2545 - mae: 21.1731 - val_loss: 723.2352 - val_mae: 21.7662\n",
      "Epoch 16/20\n",
      "1736/1736 [==============================] - 1s 640us/step - loss: 680.9803 - mae: 21.1778 - val_loss: 735.6353 - val_mae: 21.5111\n",
      "Epoch 17/20\n",
      "1736/1736 [==============================] - 1s 636us/step - loss: 680.6687 - mae: 21.1771 - val_loss: 736.2017 - val_mae: 21.4927\n",
      "Epoch 18/20\n",
      "1736/1736 [==============================] - 1s 645us/step - loss: 680.8019 - mae: 21.1787 - val_loss: 731.1693 - val_mae: 21.7041\n",
      "Epoch 19/20\n",
      "1736/1736 [==============================] - 1s 642us/step - loss: 680.6183 - mae: 21.1748 - val_loss: 749.3157 - val_mae: 21.5146\n",
      "Epoch 20/20\n",
      "1736/1736 [==============================] - 1s 648us/step - loss: 680.6898 - mae: 21.1712 - val_loss: 731.0247 - val_mae: 21.5780\n"
     ]
    }
   ],
   "source": [
    "hist2 = mm2.fit(long3_X_ss, long3_target, epochs=20, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_21 (Dense)             (None, 16)                112       \n_________________________________________________________________\ndense_22 (Dense)             (None, 32)                544       \n_________________________________________________________________\ndense_23 (Dense)             (None, 16)                528       \n_________________________________________________________________\ndense_24 (Dense)             (None, 1)                 17        \n=================================================================\nTotal params: 1,201\nTrainable params: 1,201\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nnn2 = [6, 16, 32, 16, 1]\n",
    "mm3 = create_model(nnn2)\n",
    "mm3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1736/1736 [==============================] - 1s 701us/step - loss: 1114.5123 - mae: 25.9695 - val_loss: 740.5522 - val_mae: 21.6568\n",
      "Epoch 2/20\n",
      "1736/1736 [==============================] - 1s 645us/step - loss: 683.1221 - mae: 21.2069 - val_loss: 741.5607 - val_mae: 21.5105\n",
      "Epoch 3/20\n",
      "1736/1736 [==============================] - 1s 638us/step - loss: 681.2881 - mae: 21.1807 - val_loss: 742.2895 - val_mae: 21.5256\n",
      "Epoch 4/20\n",
      "1736/1736 [==============================] - 1s 647us/step - loss: 680.5054 - mae: 21.1635 - val_loss: 733.5297 - val_mae: 21.5645\n",
      "Epoch 5/20\n",
      "1736/1736 [==============================] - 1s 642us/step - loss: 681.2075 - mae: 21.1827 - val_loss: 731.5914 - val_mae: 21.5206\n",
      "Epoch 6/20\n",
      "1736/1736 [==============================] - 1s 638us/step - loss: 680.9113 - mae: 21.1780 - val_loss: 729.3060 - val_mae: 21.5642\n",
      "Epoch 7/20\n",
      "1736/1736 [==============================] - 1s 648us/step - loss: 681.0417 - mae: 21.1846 - val_loss: 731.3064 - val_mae: 21.6591\n",
      "Epoch 8/20\n",
      "1736/1736 [==============================] - 1s 643us/step - loss: 680.5124 - mae: 21.1843 - val_loss: 780.4676 - val_mae: 21.4685\n",
      "Epoch 9/20\n",
      "1736/1736 [==============================] - 1s 646us/step - loss: 680.7589 - mae: 21.1716 - val_loss: 728.6780 - val_mae: 21.5607\n",
      "Epoch 10/20\n",
      "1736/1736 [==============================] - 1s 641us/step - loss: 680.6896 - mae: 21.1796 - val_loss: 745.7883 - val_mae: 21.4649\n",
      "Epoch 11/20\n",
      "1736/1736 [==============================] - 1s 644us/step - loss: 681.1562 - mae: 21.1809 - val_loss: 736.0900 - val_mae: 21.4953\n",
      "Epoch 12/20\n",
      "1736/1736 [==============================] - 1s 648us/step - loss: 680.6827 - mae: 21.1693 - val_loss: 741.3548 - val_mae: 21.4938\n",
      "Epoch 13/20\n",
      "1736/1736 [==============================] - 1s 644us/step - loss: 680.2523 - mae: 21.1761 - val_loss: 740.1799 - val_mae: 21.5301\n",
      "Epoch 14/20\n",
      "1736/1736 [==============================] - 1s 644us/step - loss: 680.3863 - mae: 21.1763 - val_loss: 721.9815 - val_mae: 21.7814\n",
      "Epoch 15/20\n",
      "1736/1736 [==============================] - 1s 658us/step - loss: 680.5677 - mae: 21.1732 - val_loss: 763.0908 - val_mae: 21.4596\n",
      "Epoch 16/20\n",
      "1736/1736 [==============================] - 1s 687us/step - loss: 680.6819 - mae: 21.1704 - val_loss: 754.3952 - val_mae: 21.4665\n",
      "Epoch 17/20\n",
      "1736/1736 [==============================] - 1s 678us/step - loss: 680.2723 - mae: 21.1713 - val_loss: 741.2897 - val_mae: 21.4810\n",
      "Epoch 18/20\n",
      "1736/1736 [==============================] - 1s 659us/step - loss: 680.3268 - mae: 21.1725 - val_loss: 749.0192 - val_mae: 21.4859\n",
      "Epoch 19/20\n",
      "1736/1736 [==============================] - 1s 695us/step - loss: 680.7120 - mae: 21.1689 - val_loss: 743.8503 - val_mae: 21.4679\n",
      "Epoch 20/20\n",
      "1736/1736 [==============================] - 1s 675us/step - loss: 680.5609 - mae: 21.1812 - val_loss: 756.5435 - val_mae: 21.4298\n"
     ]
    }
   ],
   "source": [
    "hist3 = mm3.fit(longp_X_ss, longp_target, epochs=20, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_8\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_27 (Dense)             (None, 8)                 40        \n_________________________________________________________________\ndense_28 (Dense)             (None, 8)                 72        \n_________________________________________________________________\ndense_29 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 121\nTrainable params: 121\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnn4 = [4,8,8,1]\n",
    "mm4 = create_model(nnn4)\n",
    "mm4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1736/1736 [==============================] - 1s 652us/step - loss: 1853.4218 - mae: 34.5761 - val_loss: 764.2430 - val_mae: 21.7612\n",
      "Epoch 2/20\n",
      "1736/1736 [==============================] - 1s 603us/step - loss: 683.1356 - mae: 21.2062 - val_loss: 734.2700 - val_mae: 21.5948\n",
      "Epoch 3/20\n",
      "1736/1736 [==============================] - 1s 608us/step - loss: 679.7584 - mae: 21.1662 - val_loss: 736.2822 - val_mae: 21.5247\n",
      "Epoch 4/20\n",
      "1736/1736 [==============================] - 1s 604us/step - loss: 679.0566 - mae: 21.1558 - val_loss: 750.0869 - val_mae: 21.4635\n",
      "Epoch 5/20\n",
      "1736/1736 [==============================] - 1s 614us/step - loss: 678.8418 - mae: 21.1460 - val_loss: 753.9050 - val_mae: 21.4312\n",
      "Epoch 6/20\n",
      "1736/1736 [==============================] - 1s 609us/step - loss: 678.7855 - mae: 21.1402 - val_loss: 735.3417 - val_mae: 21.5001\n",
      "Epoch 7/20\n",
      "1736/1736 [==============================] - 1s 598us/step - loss: 678.4727 - mae: 21.1427 - val_loss: 735.5728 - val_mae: 21.5056\n",
      "Epoch 8/20\n",
      "1736/1736 [==============================] - 1s 599us/step - loss: 678.6526 - mae: 21.1384 - val_loss: 735.3007 - val_mae: 21.4912\n",
      "Epoch 9/20\n",
      "1736/1736 [==============================] - 1s 651us/step - loss: 678.6709 - mae: 21.1450 - val_loss: 737.6357 - val_mae: 21.5376\n",
      "Epoch 10/20\n",
      "1736/1736 [==============================] - 1s 607us/step - loss: 678.6851 - mae: 21.1482 - val_loss: 745.1934 - val_mae: 21.4394\n",
      "Epoch 11/20\n",
      "1736/1736 [==============================] - 1s 606us/step - loss: 678.5518 - mae: 21.1488 - val_loss: 748.9895 - val_mae: 21.4287\n",
      "Epoch 12/20\n",
      "1736/1736 [==============================] - 1s 601us/step - loss: 678.6105 - mae: 21.1412 - val_loss: 741.1047 - val_mae: 21.5466\n",
      "Epoch 13/20\n",
      "1736/1736 [==============================] - 1s 617us/step - loss: 678.7065 - mae: 21.1469 - val_loss: 737.1573 - val_mae: 21.4867\n",
      "Epoch 14/20\n",
      "1736/1736 [==============================] - 1s 633us/step - loss: 678.4065 - mae: 21.1485 - val_loss: 748.8627 - val_mae: 21.4463\n",
      "Epoch 15/20\n",
      "1736/1736 [==============================] - 1s 615us/step - loss: 678.4224 - mae: 21.1421 - val_loss: 752.8242 - val_mae: 21.4336\n",
      "Epoch 16/20\n",
      "1736/1736 [==============================] - 1s 630us/step - loss: 678.5764 - mae: 21.1390 - val_loss: 741.8658 - val_mae: 21.5242\n",
      "Epoch 17/20\n",
      "1736/1736 [==============================] - 1s 612us/step - loss: 678.4865 - mae: 21.1422 - val_loss: 748.1143 - val_mae: 21.4413\n",
      "Epoch 18/20\n",
      "1736/1736 [==============================] - 1s 628us/step - loss: 678.6958 - mae: 21.1446 - val_loss: 735.5134 - val_mae: 21.4998\n",
      "Epoch 19/20\n",
      "1736/1736 [==============================] - 1s 617us/step - loss: 678.6409 - mae: 21.1409 - val_loss: 730.9484 - val_mae: 21.5268\n",
      "Epoch 20/20\n",
      "1736/1736 [==============================] - 1s 619us/step - loss: 678.5061 - mae: 21.1453 - val_loss: 732.9014 - val_mae: 21.5716\n"
     ]
    }
   ],
   "source": [
    "hist4 = mm4.fit(longbasic_X_ss, longbasic_target, epochs=20, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leakymodel(neuron_list, dropout=False, dropout_rate=0.2, opt=\"adam\", loss=\"mse\", **kwargs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron_list[1], kernel_initializer='normal', input_dim=neuron_list[0]))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    for i in neuron_list[2:-1]:\n",
    "        model.add(Dense(i, kernel_initializer='normal', **kwargs))\n",
    "        model.add(LeakyReLU(alpha=0.05))\n",
    "    if dropout ==True:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neuron_list[-1]))\n",
    "    #Compile\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=[\"mae\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_30 (Dense)             (None, 8)                 40        \n_________________________________________________________________\nleaky_re_lu (LeakyReLU)      (None, 8)                 0         \n_________________________________________________________________\ndense_31 (Dense)             (None, 8)                 72        \n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 8)                 0         \n_________________________________________________________________\ndense_32 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 121\nTrainable params: 121\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnn4 = [4,8,8,1]\n",
    "mm4 = create_leakymodel(nnn4)\n",
    "mm4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_10\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_33 (Dense)             (None, 8)                 40        \n_________________________________________________________________\ndense_34 (Dense)             (None, 8)                 72        \n_________________________________________________________________\ndense_35 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 121\nTrainable params: 121\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnn4 = [4,8,8,1]\n",
    "mm5 = create_tanhmodel(nnn4)\n",
    "mm5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1736/1736 [==============================] - 1s 654us/step - loss: 4299.3594 - mae: 60.5382 - val_loss: 2917.0266 - val_mae: 48.6913\n",
      "Epoch 2/20\n",
      "1736/1736 [==============================] - 1s 593us/step - loss: 2675.3650 - mae: 46.9822 - val_loss: 1816.4236 - val_mae: 38.0201\n",
      "Epoch 3/20\n",
      "1736/1736 [==============================] - 1s 579us/step - loss: 1676.0762 - mae: 36.8109 - val_loss: 1142.8120 - val_mae: 29.8051\n",
      "Epoch 4/20\n",
      "1736/1736 [==============================] - 1s 591us/step - loss: 1069.7635 - mae: 29.0746 - val_loss: 809.0660 - val_mae: 24.5662\n",
      "Epoch 5/20\n",
      "1736/1736 [==============================] - 1s 596us/step - loss: 775.9580 - mae: 24.2290 - val_loss: 719.0170 - val_mae: 22.1216\n",
      "Epoch 6/20\n",
      "1736/1736 [==============================] - 1s 612us/step - loss: 687.9399 - mae: 21.8956 - val_loss: 731.4593 - val_mae: 21.5279\n",
      "Epoch 7/20\n",
      "1736/1736 [==============================] - 1s 596us/step - loss: 677.9252 - mae: 21.2067 - val_loss: 737.6908 - val_mae: 21.4784\n",
      "Epoch 8/20\n",
      "1736/1736 [==============================] - 1s 593us/step - loss: 677.7368 - mae: 21.1504 - val_loss: 739.9570 - val_mae: 21.4648\n",
      "Epoch 9/20\n",
      "1736/1736 [==============================] - 1s 603us/step - loss: 677.7145 - mae: 21.1200 - val_loss: 738.0406 - val_mae: 21.4762\n",
      "Epoch 10/20\n",
      "1736/1736 [==============================] - 1s 604us/step - loss: 677.7355 - mae: 21.1490 - val_loss: 740.0648 - val_mae: 21.4642\n",
      "Epoch 11/20\n",
      "1736/1736 [==============================] - 1s 617us/step - loss: 677.8073 - mae: 21.1198 - val_loss: 738.4946 - val_mae: 21.4735\n",
      "Epoch 12/20\n",
      "1736/1736 [==============================] - 1s 603us/step - loss: 677.7068 - mae: 21.1331 - val_loss: 738.7546 - val_mae: 21.4718\n",
      "Epoch 13/20\n",
      "1736/1736 [==============================] - 1s 593us/step - loss: 677.7079 - mae: 21.1385 - val_loss: 739.7998 - val_mae: 21.4659\n",
      "Epoch 14/20\n",
      "1736/1736 [==============================] - 1s 604us/step - loss: 677.7234 - mae: 21.1331 - val_loss: 739.4594 - val_mae: 21.4687\n",
      "Epoch 15/20\n",
      "1736/1736 [==============================] - 1s 609us/step - loss: 677.7167 - mae: 21.1278 - val_loss: 738.9824 - val_mae: 21.4713\n",
      "Epoch 16/20\n",
      "1736/1736 [==============================] - 1s 607us/step - loss: 677.7161 - mae: 21.1329 - val_loss: 738.6580 - val_mae: 21.4738\n",
      "Epoch 17/20\n",
      "1736/1736 [==============================] - 1s 605us/step - loss: 677.7043 - mae: 21.1336 - val_loss: 738.9177 - val_mae: 21.4724\n",
      "Epoch 18/20\n",
      "1736/1736 [==============================] - 1s 626us/step - loss: 677.7194 - mae: 21.1362 - val_loss: 739.6020 - val_mae: 21.4674\n",
      "Epoch 19/20\n",
      "1736/1736 [==============================] - 1s 626us/step - loss: 677.7047 - mae: 21.1275 - val_loss: 737.7401 - val_mae: 21.4799\n",
      "Epoch 20/20\n",
      "1736/1736 [==============================] - 1s 615us/step - loss: 677.6835 - mae: 21.1551 - val_loss: 740.6841 - val_mae: 21.4614\n"
     ]
    }
   ],
   "source": [
    "hist5 = mm5.fit(longbasic_X_ss, longbasic_target, epochs=20, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neuron_list, dropout=False, dropout_rate=0.2, opt=\"adam\", loss=\"mse\", activation=\"sigmoid\",**kwargs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron_list[1], activation=activation, kernel_initializer='normal', input_dim=neuron_list[0]))\n",
    "    for i in neuron_list[2:-1]:\n",
    "        model.add(Dense(i, kernel_initializer='normal', activation=activation, **kwargs))\n",
    "    if dropout ==True:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neuron_list[-1]))\n",
    "    #Compile\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=[\"mae\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_11\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_36 (Dense)             (None, 8)                 40        \n_________________________________________________________________\ndense_37 (Dense)             (None, 8)                 72        \n_________________________________________________________________\ndense_38 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 121\nTrainable params: 121\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnn6 = [4,8,8,1]\n",
    "mm6 = create_model(nnn6)\n",
    "mm6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1736/1736 [==============================] - 1s 643us/step - loss: 5104.4307 - mae: 66.5993 - val_loss: 3687.6824 - val_mae: 55.2794\n",
      "Epoch 2/20\n",
      "1736/1736 [==============================] - 1s 584us/step - loss: 3459.0022 - mae: 53.8448 - val_loss: 2388.0693 - val_mae: 43.7918\n",
      "Epoch 3/20\n",
      "1736/1736 [==============================] - 1s 594us/step - loss: 2115.9219 - mae: 41.5524 - val_loss: 1395.6809 - val_mae: 33.1637\n",
      "Epoch 4/20\n",
      "1736/1736 [==============================] - 1s 599us/step - loss: 1291.8387 - mae: 32.1198 - val_loss: 916.8398 - val_mae: 26.4220\n",
      "Epoch 5/20\n",
      "1736/1736 [==============================] - 1s 587us/step - loss: 870.3090 - mae: 25.9864 - val_loss: 736.1306 - val_mae: 22.8602\n",
      "Epoch 6/20\n",
      "1736/1736 [==============================] - 1s 588us/step - loss: 709.3646 - mae: 22.6328 - val_loss: 722.7071 - val_mae: 21.6684\n",
      "Epoch 7/20\n",
      "1736/1736 [==============================] - 1s 593us/step - loss: 679.3220 - mae: 21.3796 - val_loss: 736.1194 - val_mae: 21.4888\n",
      "Epoch 8/20\n",
      "1736/1736 [==============================] - 1s 584us/step - loss: 677.7633 - mae: 21.1525 - val_loss: 738.5375 - val_mae: 21.4732\n",
      "Epoch 9/20\n",
      "1736/1736 [==============================] - 1s 684us/step - loss: 677.7266 - mae: 21.1435 - val_loss: 740.0139 - val_mae: 21.4645\n",
      "Epoch 10/20\n",
      "1736/1736 [==============================] - 1s 603us/step - loss: 677.7181 - mae: 21.1389 - val_loss: 740.7969 - val_mae: 21.4600\n",
      "Epoch 11/20\n",
      "1736/1736 [==============================] - 1s 623us/step - loss: 677.6927 - mae: 21.1145 - val_loss: 737.8649 - val_mae: 21.4799\n",
      "Epoch 12/20\n",
      "1736/1736 [==============================] - 1s 709us/step - loss: 677.6620 - mae: 21.1489 - val_loss: 739.3444 - val_mae: 21.4757\n",
      "Epoch 13/20\n",
      "1736/1736 [==============================] - 1s 607us/step - loss: 677.6539 - mae: 21.1288 - val_loss: 739.6888 - val_mae: 21.4714\n",
      "Epoch 14/20\n",
      "1736/1736 [==============================] - 1s 604us/step - loss: 677.6404 - mae: 21.1166 - val_loss: 737.9219 - val_mae: 21.4847\n",
      "Epoch 15/20\n",
      "1736/1736 [==============================] - 1s 597us/step - loss: 677.6368 - mae: 21.1424 - val_loss: 739.8905 - val_mae: 21.4693\n",
      "Epoch 16/20\n",
      "1736/1736 [==============================] - 1s 627us/step - loss: 677.6063 - mae: 21.1440 - val_loss: 741.6097 - val_mae: 21.4604\n",
      "Epoch 17/20\n",
      "1736/1736 [==============================] - 1s 634us/step - loss: 677.6599 - mae: 21.1178 - val_loss: 739.7005 - val_mae: 21.4733\n",
      "Epoch 18/20\n",
      "1736/1736 [==============================] - 1s 599us/step - loss: 677.6375 - mae: 21.1289 - val_loss: 739.7407 - val_mae: 21.4707\n",
      "Epoch 19/20\n",
      "1736/1736 [==============================] - 1s 602us/step - loss: 677.6490 - mae: 21.1278 - val_loss: 739.2189 - val_mae: 21.4743\n",
      "Epoch 20/20\n",
      "1736/1736 [==============================] - 1s 598us/step - loss: 677.6412 - mae: 21.1220 - val_loss: 738.2390 - val_mae: 21.4824\n"
     ]
    }
   ],
   "source": [
    "hist6sigmoid = mm6.fit(longbasic_X_ss, longbasic_target, epochs=20, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_12\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_39 (Dense)             (None, 128)               1408      \n_________________________________________________________________\ndense_40 (Dense)             (None, 256)               33024     \n_________________________________________________________________\ndense_41 (Dense)             (None, 256)               65792     \n_________________________________________________________________\ndense_42 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndense_43 (Dense)             (None, 1)                 129       \n=================================================================\nTotal params: 133,249\nTrainable params: 133,249\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnn7 = [10,128,256, 256, 128,1]\n",
    "mm7 = create_model(nnn7)\n",
    "mm7.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 1538.7242 - mae: 32.9739 - val_loss: 727.8545 - val_mae: 21.5712\n",
      "Epoch 2/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9199 - mae: 21.1827 - val_loss: 738.2849 - val_mae: 21.4747\n",
      "Epoch 3/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.7920 - mae: 21.1377 - val_loss: 738.2309 - val_mae: 21.4750\n",
      "Epoch 4/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.8273 - mae: 21.1389 - val_loss: 737.8133 - val_mae: 21.4776\n",
      "Epoch 5/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.8976 - mae: 21.1334 - val_loss: 734.8254 - val_mae: 21.4981\n",
      "Epoch 6/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.8268 - mae: 21.1301 - val_loss: 733.9778 - val_mae: 21.5044\n",
      "Epoch 7/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.9115 - mae: 21.1413 - val_loss: 738.5854 - val_mae: 21.4729\n",
      "Epoch 8/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.8875 - mae: 21.1368 - val_loss: 740.2629 - val_mae: 21.4631\n",
      "Epoch 9/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9188 - mae: 21.1338 - val_loss: 736.6845 - val_mae: 21.4848\n",
      "Epoch 10/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9083 - mae: 21.1365 - val_loss: 736.0370 - val_mae: 21.4894\n",
      "Epoch 11/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9114 - mae: 21.1367 - val_loss: 739.2300 - val_mae: 21.4691\n",
      "Epoch 12/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.8926 - mae: 21.1374 - val_loss: 741.7855 - val_mae: 21.4550\n",
      "Epoch 13/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.8922 - mae: 21.1371 - val_loss: 743.7636 - val_mae: 21.4484\n",
      "Epoch 14/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9539 - mae: 21.1297 - val_loss: 739.6240 - val_mae: 21.4668\n",
      "Epoch 15/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.8245 - mae: 21.1317 - val_loss: 739.2137 - val_mae: 21.4691\n",
      "Epoch 16/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.8607 - mae: 21.1343 - val_loss: 740.6792 - val_mae: 21.4607\n",
      "Epoch 17/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9199 - mae: 21.1298 - val_loss: 735.7040 - val_mae: 21.4917\n",
      "Epoch 18/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9299 - mae: 21.1426 - val_loss: 739.2217 - val_mae: 21.4691\n",
      "Epoch 19/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.8761 - mae: 21.1378 - val_loss: 740.7125 - val_mae: 21.4605\n",
      "Epoch 20/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.7679 - mae: 21.1304 - val_loss: 739.0070 - val_mae: 21.4704\n"
     ]
    }
   ],
   "source": [
    "hist7sigmoid = mm7.fit(long_X_ss, long_target, epochs=20, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_13\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_44 (Dense)             (None, 128)               1280      \n_________________________________________________________________\ndense_45 (Dense)             (None, 256)               33024     \n_________________________________________________________________\ndense_46 (Dense)             (None, 256)               65792     \n_________________________________________________________________\ndense_47 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndense_48 (Dense)             (None, 1)                 129       \n=================================================================\nTotal params: 133,121\nTrainable params: 133,121\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnn8 = [9,128,256, 256, 128,1]\n",
    "mm8 = create_model(nnn8)\n",
    "mm8.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 1545.6024 - mae: 33.0856 - val_loss: 727.4062 - val_mae: 21.5774\n",
      "Epoch 2/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9192 - mae: 21.1829 - val_loss: 738.0740 - val_mae: 21.4760\n",
      "Epoch 3/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.7712 - mae: 21.1492 - val_loss: 746.7595 - val_mae: 21.4391\n",
      "Epoch 4/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.8400 - mae: 21.1183 - val_loss: 732.8257 - val_mae: 21.5134\n",
      "Epoch 5/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9272 - mae: 21.1381 - val_loss: 738.2029 - val_mae: 21.4752\n",
      "Epoch 6/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9205 - mae: 21.1342 - val_loss: 738.4628 - val_mae: 21.4736\n",
      "Epoch 7/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.9208 - mae: 21.1439 - val_loss: 745.9486 - val_mae: 21.4415\n",
      "Epoch 8/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.9302 - mae: 21.1311 - val_loss: 740.4982 - val_mae: 21.4617\n",
      "Epoch 9/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.8630 - mae: 21.1383 - val_loss: 742.5043 - val_mae: 21.4526\n",
      "Epoch 10/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.8707 - mae: 21.1280 - val_loss: 735.5526 - val_mae: 21.4928\n",
      "Epoch 11/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.9412 - mae: 21.1406 - val_loss: 736.1028 - val_mae: 21.4889\n",
      "Epoch 12/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.8672 - mae: 21.1382 - val_loss: 741.9844 - val_mae: 21.4543\n",
      "Epoch 13/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.9002 - mae: 21.1300 - val_loss: 738.0832 - val_mae: 21.4759\n",
      "Epoch 14/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.9247 - mae: 21.1415 - val_loss: 738.7444 - val_mae: 21.4719\n",
      "Epoch 15/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.8844 - mae: 21.1322 - val_loss: 737.7936 - val_mae: 21.4777\n",
      "Epoch 16/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.8502 - mae: 21.1363 - val_loss: 740.1575 - val_mae: 21.4637\n",
      "Epoch 17/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.9491 - mae: 21.1364 - val_loss: 739.3118 - val_mae: 21.4686\n",
      "Epoch 18/20\n",
      "1736/1736 [==============================] - 3s 1ms/step - loss: 677.8776 - mae: 21.1306 - val_loss: 737.0194 - val_mae: 21.4826\n",
      "Epoch 19/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.9415 - mae: 21.1429 - val_loss: 745.9681 - val_mae: 21.4414\n",
      "Epoch 20/20\n",
      "1736/1736 [==============================] - 3s 2ms/step - loss: 677.9040 - mae: 21.1269 - val_loss: 740.5897 - val_mae: 21.4612\n"
     ]
    }
   ],
   "source": [
    "hist8gram = mm8.fit(longgram_X_ss, longgram_target, epochs=20, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_lemma(corpus, tool, language = 'en', pos_to_remove = ['PUNCT','NUM'], ent_to_remove = ['PERSON','ORG'], stop_words_to_remove= False, lowercase = True, regex=True):\n",
    "    \"\"\"\n",
    "    tool: one of two strings - 'spacy' or 'NLTK'\n",
    "    languages (string ISO code): supports 'en', 'fi' or 'zh'\n",
    "    pos_to_remove (list): part-of-speech tag from spacy\n",
    "    ent_to_remove (list): entities from spacy\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    tokenized_corpus = []\n",
    "    if tool == 'spacy':\n",
    "        if language == 'en':\n",
    "            sc = spacy.load('en_core_web_sm')\n",
    "            for doc in tqdm(sc.pipe(corpus, disable=[\"textcat\", \"custom\"])):\n",
    "                if stop_words_to_remove:\n",
    "                    doc_list = [word.lemma_ for word in doc if word.pos_ not in pos_to_remove if word.ent_type_ not in ent_to_remove if not word.is_stop]\n",
    "                else: \n",
    "                    doc_list = [word.lemma_ for word in doc if word.pos_ not in pos_to_remove if word.ent_type_ not in ent_to_remove]\n",
    "\n",
    "                if lowercase:\n",
    "                    doc_list = [word.lower() for word in doc_list]\n",
    "\n",
    "                tokenized_corpus.append(doc_list)\n",
    "        if language == 'zh':\n",
    "            sc = spacy.load('zh_core_web_sm')\n",
    "            for doc in tqdm(sc.pipe(corpus, disable=[\"textcat\", \"custom\"])):\n",
    "                if stop_words_to_remove:\n",
    "                    doc_list = [word.lemma_ for word in doc if word.pos_ not in ['PUNCT'] if not word.is_stop]\n",
    "                else: \n",
    "                    doc_list = [word.lemma_ for word in doc if word.pos_ not in ['PUNCT']]\n",
    "                \n",
    "                tokenized_corpus.append(doc_list)\n",
    "        if language == 'fi':\n",
    "            sc = spacy.load('xx_sent_ud_sm')\n",
    "            for doc in tqdm(sc.pipe(corpus, disable=[\"textcat\", \"custom\"])): \n",
    "                doc_list = [word.lemma_ for word in doc]\n",
    "                \n",
    "                if lowercase:\n",
    "                    doc_list = [word.lower() for word in doc_list]\n",
    "\n",
    "                tokenized_corpus.append(doc_list)\n",
    "    \n",
    "        \n",
    "        \n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_preprocessor_lemma(df,lang, tool=\"spacy\", pos_list=[], ent_list=[], stopwords=False, lowercase=True, let=True, letandnum=False):\n",
    "    if lang == \"en\":\n",
    "        word_vect = KeyedVectors.load('en_vectors_lemma.kv')\n",
    "\n",
    "        df[\"pt\"] = tokenization_lemma(df[\"translation\"], tool = tool, language = lang, pos_to_remove = pos_list, ent_to_remove = ent_list, stop_words_to_remove= stopwords, lowercase = lowercase)\n",
    "\n",
    "        df[\"ref\"] = tokenization_lemma(df[\"reference\"], tool = tool, language = lang, pos_to_remove = pos_list, ent_to_remove = ent_list, stop_words_to_remove= stopwords, lowercase = lowercase)\n",
    "\n",
    "    elif lang == \"fi\":\n",
    "        word_vect = KeyedVectors.load('fi_vectors.kv')\n",
    "\n",
    "        df[\"pt\"] = tokenization_lemma(df[\"translation\"], tool = tool, language = lang, lowercase = lowercase)\n",
    "        df[\"pt\"] = match_regex(df[\"pt\"].tolist(), letters = let, letters_and_numbers = letandnum)\n",
    "\n",
    "        df[\"ref\"] = tokenization_lemma(df[\"reference\"], tool = tool, language = lang, lowercase = lowercase)\n",
    "        df[\"ref\"] = match_regex(df[\"ref\"].tolist(), letters = let, letters_and_numbers = letandnum)\n",
    "\n",
    "    elif lang == \"zh\":\n",
    "        word_vect = KeyedVectors.load('zh_vectors.kv')\n",
    "\n",
    "        df[\"pt\"] = tokenization_lemma(df[\"translation\"].tolist(), tool = tool, language = lang, stop_words_to_remove= stopwords)\n",
    "        df[\"pt\"] = chinese_regex(df[\"pt\"].tolist(), chinese= let, chinese_and_numbers = letandnum)\n",
    "\n",
    "        df[\"ref\"] = tokenization_lemma(df[\"reference\"].tolist(), tool = tool, language = lang, stop_words_to_remove= stopwords)\n",
    "        df[\"ref\"] = chinese_regex(df[\"ref\"].tolist(), chinese= let, chinese_and_numbers = letandnum)\n",
    "\n",
    "    else:\n",
    "        return \"ooppsiiee, language not defined\"\n",
    "\n",
    "    for mode in [\"precision\", \"recall\"]:\n",
    "        for i in range(1,5):\n",
    "            df[str(i)+\"gram-\"+ mode] = df.apply(lambda x: ngram_calc(x.pt, x.ref, mode=mode, ngram=i), axis =1)\n",
    "\n",
    "    df['wmdist'] = df.apply(lambda x: word_vect.wmdistance(x[\"pt\"],x[\"ref\"]), axis=1)\n",
    "    \n",
    "    df[\"pt_len\"] = df['pt'].str.len()\n",
    "    df[\"ref_len\"] = df['ref'].str.len()\n",
    "    df[\"len_diff\"] = df[\"pt_len\"] - df[\"ref_len\"]\n",
    "\n",
    "    df = df.drop([\"reference\", \"translation\", \"annotators\"], axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "77688it [02:56, 439.73it/s]\n",
      "77688it [02:57, 438.07it/s]\n"
     ]
    }
   ],
   "source": [
    "en_lemma = the_preprocessor_lemma(en_df, lang=\"en\", pos_list=['PUNCT','NUM'], ent_list=['PERSON','ORG'], stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              source   z-score  avg-score  \\\n",
       "0  Uchopíte pak zbraň mezi své předloktí a rameno... -0.675383  60.000000   \n",
       "1  Ale je-li New York změna, pak je to také znovu... -0.829403  44.000000   \n",
       "2  Dlouho a intenzivně jsem během léta přemýšlel,...  0.803185  96.500000   \n",
       "3         Najdou si jiný způsob, jak někde podvádět.  0.563149  90.500000   \n",
       "4  Zpráva o výměně v čele prezidentovy administra...  0.021549  74.666667   \n",
       "\n",
       "                                                  pt  \\\n",
       "0  [grasp, gun, forearm, shoulder, hit, face, fre...   \n",
       "1                   [new, york, change, reinvention]   \n",
       "2  [think, long, hard, course, summer, improve, t...   \n",
       "3                               [find, way, defraud]   \n",
       "4  [news, replacement, president, office, come, s...   \n",
       "\n",
       "                                                 ref  1gram-precision  \\\n",
       "0  [grab, weapon, forearm, shoulder, hit, face, f...         0.750000   \n",
       "1                   [new, york, change, rediscovery]         0.750000   \n",
       "2  [think, summer, improve, depth, need, high, le...         0.583333   \n",
       "3                                 [find, way, cheat]         0.666667   \n",
       "4  [report, replacement, president, administratio...         0.333333   \n",
       "\n",
       "   2gram-precision  3gram-precision  4gram-precision  1gram-recall  \\\n",
       "0         0.714286         0.666667              0.6      0.750000   \n",
       "1         0.666667         0.500000              0.0      0.750000   \n",
       "2         0.272727         0.000000              0.0      1.000000   \n",
       "3         0.500000         0.000000              0.0      0.666667   \n",
       "4         0.125000         0.000000              0.0      0.300000   \n",
       "\n",
       "   2gram-recall  3gram-recall  4gram-recall    wmdist  pt_len  ref_len  \\\n",
       "0      0.714286      0.666667           0.6  0.231686       8        8   \n",
       "1      0.666667      0.500000           0.0  0.144161       4        4   \n",
       "2      0.500000      0.000000           0.0  0.516081      12        7   \n",
       "3      0.500000      0.000000           0.0  0.316397       3        3   \n",
       "4      0.111111      0.000000           0.0  0.814173       9       10   \n",
       "\n",
       "   len_diff  \n",
       "0         0  \n",
       "1         0  \n",
       "2         5  \n",
       "3         0  \n",
       "4        -1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>z-score</th>\n      <th>avg-score</th>\n      <th>pt</th>\n      <th>ref</th>\n      <th>1gram-precision</th>\n      <th>2gram-precision</th>\n      <th>3gram-precision</th>\n      <th>4gram-precision</th>\n      <th>1gram-recall</th>\n      <th>2gram-recall</th>\n      <th>3gram-recall</th>\n      <th>4gram-recall</th>\n      <th>wmdist</th>\n      <th>pt_len</th>\n      <th>ref_len</th>\n      <th>len_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Uchopíte pak zbraň mezi své předloktí a rameno...</td>\n      <td>-0.675383</td>\n      <td>60.000000</td>\n      <td>[grasp, gun, forearm, shoulder, hit, face, fre...</td>\n      <td>[grab, weapon, forearm, shoulder, hit, face, f...</td>\n      <td>0.750000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.6</td>\n      <td>0.750000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.6</td>\n      <td>0.231686</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ale je-li New York změna, pak je to také znovu...</td>\n      <td>-0.829403</td>\n      <td>44.000000</td>\n      <td>[new, york, change, reinvention]</td>\n      <td>[new, york, change, rediscovery]</td>\n      <td>0.750000</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.750000</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.144161</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dlouho a intenzivně jsem během léta přemýšlel,...</td>\n      <td>0.803185</td>\n      <td>96.500000</td>\n      <td>[think, long, hard, course, summer, improve, t...</td>\n      <td>[think, summer, improve, depth, need, high, le...</td>\n      <td>0.583333</td>\n      <td>0.272727</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.516081</td>\n      <td>12</td>\n      <td>7</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Najdou si jiný způsob, jak někde podvádět.</td>\n      <td>0.563149</td>\n      <td>90.500000</td>\n      <td>[find, way, defraud]</td>\n      <td>[find, way, cheat]</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.316397</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Zpráva o výměně v čele prezidentovy administra...</td>\n      <td>0.021549</td>\n      <td>74.666667</td>\n      <td>[news, replacement, president, office, come, s...</td>\n      <td>[report, replacement, president, administratio...</td>\n      <td>0.333333</td>\n      <td>0.125000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.300000</td>\n      <td>0.111111</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.814173</td>\n      <td>9</td>\n      <td>10</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "en_lemma.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              source   z-score  avg-score  \\\n",
       "0  Uchopíte pak zbraň mezi své předloktí a rameno... -0.675383  60.000000   \n",
       "1  Ale je-li New York změna, pak je to také znovu... -0.829403  44.000000   \n",
       "2  Dlouho a intenzivně jsem během léta přemýšlel,...  0.803185  96.500000   \n",
       "3         Najdou si jiný způsob, jak někde podvádět.  0.563149  90.500000   \n",
       "4  Zpráva o výměně v čele prezidentovy administra...  0.021549  74.666667   \n",
       "\n",
       "                                                  pt  \\\n",
       "0  [grasp, gun, forearm, shoulder, hit, face, fre...   \n",
       "1                   [new, york, change, reinvention]   \n",
       "2  [think, long, hard, course, summer, improve, t...   \n",
       "3                               [find, way, defraud]   \n",
       "4  [news, replacement, president, office, come, s...   \n",
       "\n",
       "                                                 ref  1gram-precision  \\\n",
       "0  [grab, weapon, forearm, shoulder, hit, face, f...         0.750000   \n",
       "1                   [new, york, change, rediscovery]         0.750000   \n",
       "2  [think, summer, improve, depth, need, high, le...         0.583333   \n",
       "3                                 [find, way, cheat]         0.666667   \n",
       "4  [report, replacement, president, administratio...         0.333333   \n",
       "\n",
       "   2gram-precision  3gram-precision  4gram-precision  1gram-recall  \\\n",
       "0         0.714286         0.666667              0.6      0.750000   \n",
       "1         0.666667         0.500000              0.0      0.750000   \n",
       "2         0.272727         0.000000              0.0      1.000000   \n",
       "3         0.500000         0.000000              0.0      0.666667   \n",
       "4         0.125000         0.000000              0.0      0.300000   \n",
       "\n",
       "   2gram-recall  3gram-recall  4gram-recall    wmdist  pt_len  ref_len  \\\n",
       "0      0.714286      0.666667           0.6  0.205264       8        8   \n",
       "1      0.666667      0.500000           0.0  0.281045       4        4   \n",
       "2      0.500000      0.000000           0.0  0.488398      12        7   \n",
       "3      0.500000      0.000000           0.0  0.245984       3        3   \n",
       "4      0.111111      0.000000           0.0  0.803094       9       10   \n",
       "\n",
       "   len_diff  \n",
       "0         0  \n",
       "1         0  \n",
       "2         5  \n",
       "3         0  \n",
       "4        -1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>z-score</th>\n      <th>avg-score</th>\n      <th>pt</th>\n      <th>ref</th>\n      <th>1gram-precision</th>\n      <th>2gram-precision</th>\n      <th>3gram-precision</th>\n      <th>4gram-precision</th>\n      <th>1gram-recall</th>\n      <th>2gram-recall</th>\n      <th>3gram-recall</th>\n      <th>4gram-recall</th>\n      <th>wmdist</th>\n      <th>pt_len</th>\n      <th>ref_len</th>\n      <th>len_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Uchopíte pak zbraň mezi své předloktí a rameno...</td>\n      <td>-0.675383</td>\n      <td>60.000000</td>\n      <td>[grasp, gun, forearm, shoulder, hit, face, fre...</td>\n      <td>[grab, weapon, forearm, shoulder, hit, face, f...</td>\n      <td>0.750000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.6</td>\n      <td>0.750000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.6</td>\n      <td>0.205264</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ale je-li New York změna, pak je to také znovu...</td>\n      <td>-0.829403</td>\n      <td>44.000000</td>\n      <td>[new, york, change, reinvention]</td>\n      <td>[new, york, change, rediscovery]</td>\n      <td>0.750000</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.750000</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.281045</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dlouho a intenzivně jsem během léta přemýšlel,...</td>\n      <td>0.803185</td>\n      <td>96.500000</td>\n      <td>[think, long, hard, course, summer, improve, t...</td>\n      <td>[think, summer, improve, depth, need, high, le...</td>\n      <td>0.583333</td>\n      <td>0.272727</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.488398</td>\n      <td>12</td>\n      <td>7</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Najdou si jiný způsob, jak někde podvádět.</td>\n      <td>0.563149</td>\n      <td>90.500000</td>\n      <td>[find, way, defraud]</td>\n      <td>[find, way, cheat]</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.245984</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Zpráva o výměně v čele prezidentovy administra...</td>\n      <td>0.021549</td>\n      <td>74.666667</td>\n      <td>[news, replacement, president, office, come, s...</td>\n      <td>[report, replacement, president, administratio...</td>\n      <td>0.333333</td>\n      <td>0.125000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.300000</td>\n      <td>0.111111</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.803094</td>\n      <td>9</td>\n      <td>10</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "en_lemma.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data ready for NN \n",
    "long_en_lemma = en_lemma[(en_lemma[\"pt_len\"]>3) & (en_lemma[\"ref_len\"]>3)]\n",
    "\n",
    "# data, target\n",
    "long_target_lemma = long_en_lemma[\"avg-score\"]\n",
    "long_X_lemma = long_en_lemma.drop([\"source\",\"avg-score\",\"z-score\",\"pt\",\"ref\",\"pt_len\",\"ref_len\"], axis=1)\n",
    "\n",
    "# standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sscaler = StandardScaler()\n",
    "long_X_ss_lemma = sscaler.fit_transform(long_X_lemma)\n",
    "\n",
    "# train-test-split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.20, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   1gram-precision  2gram-precision  3gram-precision  4gram-precision  \\\n",
       "0         0.750000         0.714286         0.666667              0.6   \n",
       "1         0.750000         0.666667         0.500000              0.0   \n",
       "2         0.583333         0.272727         0.000000              0.0   \n",
       "4         0.333333         0.125000         0.000000              0.0   \n",
       "5         0.500000         0.076923         0.000000              0.0   \n",
       "\n",
       "   1gram-recall  2gram-recall  3gram-recall  4gram-recall    wmdist  len_diff  \n",
       "0      0.750000      0.714286      0.666667           0.6  0.205264         0  \n",
       "1      0.750000      0.666667      0.500000           0.0  0.281045         0  \n",
       "2      1.000000      0.500000      0.000000           0.0  0.488398         5  \n",
       "4      0.300000      0.111111      0.000000           0.0  0.803094        -1  \n",
       "5      0.466667      0.071429      0.000000           0.0  0.598684        -1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1gram-precision</th>\n      <th>2gram-precision</th>\n      <th>3gram-precision</th>\n      <th>4gram-precision</th>\n      <th>1gram-recall</th>\n      <th>2gram-recall</th>\n      <th>3gram-recall</th>\n      <th>4gram-recall</th>\n      <th>wmdist</th>\n      <th>len_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.750000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.6</td>\n      <td>0.750000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.6</td>\n      <td>0.205264</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.750000</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.750000</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.281045</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.583333</td>\n      <td>0.272727</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.488398</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.333333</td>\n      <td>0.125000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.300000</td>\n      <td>0.111111</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.803094</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.500000</td>\n      <td>0.076923</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.466667</td>\n      <td>0.071429</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.598684</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "long_X_lemma.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_14\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_49 (Dense)             (None, 32)                352       \n_________________________________________________________________\ndense_50 (Dense)             (None, 64)                2112      \n_________________________________________________________________\ndense_51 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndense_52 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 4,577\nTrainable params: 4,577\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnn9 = [10, 32, 64, 32,1]\n",
    "mm9 = create_model(nnn9)\n",
    "mm9.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "1736/1736 [==============================] - 1s 845us/step - loss: 600.4567 - mae: 19.5408 - val_loss: 760.3413 - val_mae: 21.5505\n",
      "Epoch 2/50\n",
      "1736/1736 [==============================] - 1s 819us/step - loss: 600.7595 - mae: 19.5346 - val_loss: 744.7983 - val_mae: 21.4717\n",
      "Epoch 3/50\n",
      "1736/1736 [==============================] - 1s 831us/step - loss: 600.3468 - mae: 19.5376 - val_loss: 729.9000 - val_mae: 21.4314\n",
      "Epoch 4/50\n",
      "1736/1736 [==============================] - 1s 776us/step - loss: 600.2928 - mae: 19.5319 - val_loss: 714.9358 - val_mae: 21.4704\n",
      "Epoch 5/50\n",
      "1736/1736 [==============================] - 1s 781us/step - loss: 600.1994 - mae: 19.5309 - val_loss: 731.9001 - val_mae: 21.4293\n",
      "Epoch 6/50\n",
      "1736/1736 [==============================] - 1s 765us/step - loss: 600.3156 - mae: 19.5329 - val_loss: 728.4163 - val_mae: 21.4244\n",
      "Epoch 7/50\n",
      "1736/1736 [==============================] - 1s 793us/step - loss: 600.1313 - mae: 19.5354 - val_loss: 739.6371 - val_mae: 21.4411\n",
      "Epoch 8/50\n",
      "1736/1736 [==============================] - 1s 780us/step - loss: 600.2996 - mae: 19.5329 - val_loss: 759.4083 - val_mae: 21.5336\n",
      "Epoch 9/50\n",
      "1736/1736 [==============================] - 1s 770us/step - loss: 599.7479 - mae: 19.5253 - val_loss: 767.1689 - val_mae: 21.5738\n",
      "Epoch 10/50\n",
      "1736/1736 [==============================] - 1s 743us/step - loss: 599.9838 - mae: 19.5189 - val_loss: 750.0636 - val_mae: 21.4794\n",
      "Epoch 11/50\n",
      "1736/1736 [==============================] - 1s 784us/step - loss: 599.8187 - mae: 19.5232 - val_loss: 754.8571 - val_mae: 21.4987\n",
      "Epoch 12/50\n",
      "1736/1736 [==============================] - 1s 819us/step - loss: 599.5159 - mae: 19.5112 - val_loss: 735.4945 - val_mae: 21.4224\n",
      "Epoch 13/50\n",
      "1736/1736 [==============================] - 1s 785us/step - loss: 599.8365 - mae: 19.5249 - val_loss: 747.9913 - val_mae: 21.4587\n",
      "Epoch 14/50\n",
      "1736/1736 [==============================] - 1s 793us/step - loss: 599.5678 - mae: 19.5061 - val_loss: 756.9617 - val_mae: 21.5021\n",
      "Epoch 15/50\n",
      "1736/1736 [==============================] - 1s 816us/step - loss: 599.3967 - mae: 19.5046 - val_loss: 741.1923 - val_mae: 21.4374\n",
      "Epoch 16/50\n",
      "1736/1736 [==============================] - 1s 769us/step - loss: 599.4495 - mae: 19.5157 - val_loss: 753.0056 - val_mae: 21.4709\n",
      "Epoch 17/50\n",
      "1736/1736 [==============================] - 1s 779us/step - loss: 599.2441 - mae: 19.5025 - val_loss: 732.2288 - val_mae: 21.3944\n",
      "Epoch 18/50\n",
      "1736/1736 [==============================] - 1s 797us/step - loss: 599.1481 - mae: 19.5080 - val_loss: 746.8910 - val_mae: 21.4391\n",
      "Epoch 19/50\n",
      "1736/1736 [==============================] - 1s 824us/step - loss: 599.4152 - mae: 19.5073 - val_loss: 735.5704 - val_mae: 21.3961\n",
      "Epoch 20/50\n",
      "1736/1736 [==============================] - 1s 793us/step - loss: 599.2321 - mae: 19.5061 - val_loss: 737.0499 - val_mae: 21.4001\n",
      "Epoch 21/50\n",
      "1736/1736 [==============================] - 1s 779us/step - loss: 599.0579 - mae: 19.4997 - val_loss: 730.3453 - val_mae: 21.3772\n",
      "Epoch 22/50\n",
      "1736/1736 [==============================] - 1s 785us/step - loss: 599.1197 - mae: 19.4993 - val_loss: 735.3074 - val_mae: 21.3937\n",
      "Epoch 23/50\n",
      "1736/1736 [==============================] - 1s 765us/step - loss: 598.7861 - mae: 19.4981 - val_loss: 724.5201 - val_mae: 21.3686\n",
      "Epoch 24/50\n",
      "1736/1736 [==============================] - 1s 812us/step - loss: 599.0322 - mae: 19.4990 - val_loss: 756.1490 - val_mae: 21.4816\n",
      "Epoch 25/50\n",
      "1736/1736 [==============================] - 2s 869us/step - loss: 598.9405 - mae: 19.4963 - val_loss: 727.6889 - val_mae: 21.3724\n",
      "Epoch 26/50\n",
      "1736/1736 [==============================] - 1s 837us/step - loss: 598.9059 - mae: 19.4945 - val_loss: 753.4371 - val_mae: 21.4659\n",
      "Epoch 27/50\n",
      "1736/1736 [==============================] - 1s 786us/step - loss: 598.5803 - mae: 19.4950 - val_loss: 728.7859 - val_mae: 21.3600\n",
      "Epoch 28/50\n",
      "1736/1736 [==============================] - 1s 815us/step - loss: 598.8417 - mae: 19.4903 - val_loss: 746.7539 - val_mae: 21.4282\n",
      "Epoch 29/50\n",
      "1736/1736 [==============================] - 1s 773us/step - loss: 598.7101 - mae: 19.4879 - val_loss: 726.7380 - val_mae: 21.3708\n",
      "Epoch 30/50\n",
      "1736/1736 [==============================] - 1s 761us/step - loss: 598.5433 - mae: 19.4844 - val_loss: 727.8623 - val_mae: 21.3698\n",
      "Epoch 31/50\n",
      "1736/1736 [==============================] - 1s 786us/step - loss: 598.6672 - mae: 19.5011 - val_loss: 749.5575 - val_mae: 21.4320\n",
      "Epoch 32/50\n",
      "1736/1736 [==============================] - 1s 837us/step - loss: 598.3283 - mae: 19.4839 - val_loss: 733.8802 - val_mae: 21.3954\n",
      "Epoch 33/50\n",
      "1736/1736 [==============================] - 1s 757us/step - loss: 598.5021 - mae: 19.4942 - val_loss: 732.9977 - val_mae: 21.3586\n",
      "Epoch 34/50\n",
      "1736/1736 [==============================] - 1s 756us/step - loss: 598.5079 - mae: 19.4859 - val_loss: 721.9750 - val_mae: 21.3547\n",
      "Epoch 35/50\n",
      "1736/1736 [==============================] - 1s 782us/step - loss: 598.4625 - mae: 19.4864 - val_loss: 735.3970 - val_mae: 21.3621\n",
      "Epoch 36/50\n",
      "1736/1736 [==============================] - 1s 780us/step - loss: 598.2119 - mae: 19.4815 - val_loss: 736.0764 - val_mae: 21.3687\n",
      "Epoch 37/50\n",
      "1736/1736 [==============================] - 1s 783us/step - loss: 598.4853 - mae: 19.4799 - val_loss: 742.1601 - val_mae: 21.3900\n",
      "Epoch 38/50\n",
      "1736/1736 [==============================] - 2s 891us/step - loss: 598.4668 - mae: 19.4836 - val_loss: 734.8605 - val_mae: 21.3573\n",
      "Epoch 39/50\n",
      "1736/1736 [==============================] - 1s 822us/step - loss: 598.2112 - mae: 19.4787 - val_loss: 715.4260 - val_mae: 21.3668\n",
      "Epoch 40/50\n",
      "1736/1736 [==============================] - 1s 767us/step - loss: 598.4270 - mae: 19.4852 - val_loss: 732.6418 - val_mae: 21.3581\n",
      "Epoch 41/50\n",
      "1736/1736 [==============================] - 1s 785us/step - loss: 598.2473 - mae: 19.4749 - val_loss: 721.6215 - val_mae: 21.3396\n",
      "Epoch 42/50\n",
      "1736/1736 [==============================] - 1s 783us/step - loss: 598.0424 - mae: 19.4744 - val_loss: 733.3722 - val_mae: 21.3485\n",
      "Epoch 43/50\n",
      "1736/1736 [==============================] - 1s 835us/step - loss: 598.2074 - mae: 19.4743 - val_loss: 726.8646 - val_mae: 21.3407\n",
      "Epoch 44/50\n",
      "1736/1736 [==============================] - 1s 780us/step - loss: 598.2122 - mae: 19.4809 - val_loss: 741.0073 - val_mae: 21.3838\n",
      "Epoch 45/50\n",
      "1736/1736 [==============================] - 1s 783us/step - loss: 598.2564 - mae: 19.4773 - val_loss: 732.5114 - val_mae: 21.3469\n",
      "Epoch 46/50\n",
      "1736/1736 [==============================] - 1s 786us/step - loss: 598.3172 - mae: 19.4793 - val_loss: 742.9666 - val_mae: 21.4014\n",
      "Epoch 47/50\n",
      "1736/1736 [==============================] - 1s 827us/step - loss: 598.3922 - mae: 19.4794 - val_loss: 724.1582 - val_mae: 21.3453\n",
      "Epoch 48/50\n",
      "1736/1736 [==============================] - 1s 850us/step - loss: 598.0386 - mae: 19.4743 - val_loss: 722.4859 - val_mae: 21.3335\n",
      "Epoch 49/50\n",
      "1736/1736 [==============================] - 1s 829us/step - loss: 597.9789 - mae: 19.4758 - val_loss: 743.1395 - val_mae: 21.4018\n",
      "Epoch 50/50\n",
      "1736/1736 [==============================] - 1s 764us/step - loss: 598.0181 - mae: 19.4687 - val_loss: 732.5832 - val_mae: 21.3545\n"
     ]
    }
   ],
   "source": [
    "hist9lemma = mm9.fit(long_X_ss_lemma, long_target_lemma, epochs=50, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data ready for NN \n",
    "#en_lemma = en_lemma[(en_lemma[\"pt_len\"]>3) & (en_lemma[\"ref_len\"]>3)]\n",
    "\n",
    "en_lemma = en_lemma.replace(np.inf, np.nan)\n",
    "en_lemma = en_lemma.dropna()\n",
    "\n",
    " \n",
    "# data, target\n",
    "target_lemma = en_lemma[\"avg-score\"]\n",
    "X_lemma = en_lemma.drop([\"source\",\"avg-score\",\"z-score\",\"pt\",\"ref\",\"pt_len\",\"ref_len\"], axis=1)\n",
    "\n",
    "# standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sscaler = StandardScaler()\n",
    "X_ss_lemma = sscaler.fit_transform(X_lemma)\n",
    "\n",
    "# train-test-split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.20, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   1gram-precision  2gram-precision  3gram-precision  4gram-precision  \\\n",
       "0         0.750000         0.714286         0.666667              0.6   \n",
       "1         0.750000         0.666667         0.500000              0.0   \n",
       "2         0.583333         0.272727         0.000000              0.0   \n",
       "3         0.666667         0.500000         0.000000              0.0   \n",
       "4         0.333333         0.125000         0.000000              0.0   \n",
       "\n",
       "   1gram-recall  2gram-recall  3gram-recall  4gram-recall    wmdist  len_diff  \n",
       "0      0.750000      0.714286      0.666667           0.6  0.231686         0  \n",
       "1      0.750000      0.666667      0.500000           0.0  0.144161         0  \n",
       "2      1.000000      0.500000      0.000000           0.0  0.516081         5  \n",
       "3      0.666667      0.500000      0.000000           0.0  0.316397         0  \n",
       "4      0.300000      0.111111      0.000000           0.0  0.814173        -1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1gram-precision</th>\n      <th>2gram-precision</th>\n      <th>3gram-precision</th>\n      <th>4gram-precision</th>\n      <th>1gram-recall</th>\n      <th>2gram-recall</th>\n      <th>3gram-recall</th>\n      <th>4gram-recall</th>\n      <th>wmdist</th>\n      <th>len_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.750000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.6</td>\n      <td>0.750000</td>\n      <td>0.714286</td>\n      <td>0.666667</td>\n      <td>0.6</td>\n      <td>0.231686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.750000</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.750000</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.144161</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.583333</td>\n      <td>0.272727</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.516081</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.316397</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.333333</td>\n      <td>0.125000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.300000</td>\n      <td>0.111111</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.814173</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "X_lemma.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "1938/1938 [==============================] - 1s 734us/step - loss: 620.6326 - mae: 19.9018 - val_loss: 733.3951 - val_mae: 21.5178\n",
      "Epoch 2/50\n",
      "1938/1938 [==============================] - 1s 752us/step - loss: 620.2969 - mae: 19.9048 - val_loss: 739.5024 - val_mae: 21.5120\n",
      "Epoch 3/50\n",
      "1938/1938 [==============================] - 1s 741us/step - loss: 620.3938 - mae: 19.9040 - val_loss: 743.7934 - val_mae: 21.5365\n",
      "Epoch 4/50\n",
      "1938/1938 [==============================] - 1s 718us/step - loss: 620.1814 - mae: 19.9006 - val_loss: 749.7239 - val_mae: 21.5414\n",
      "Epoch 5/50\n",
      "1938/1938 [==============================] - 1s 725us/step - loss: 620.0677 - mae: 19.9060 - val_loss: 737.4897 - val_mae: 21.5096\n",
      "Epoch 6/50\n",
      "1938/1938 [==============================] - 1s 731us/step - loss: 620.0902 - mae: 19.8984 - val_loss: 738.6615 - val_mae: 21.5094\n",
      "Epoch 7/50\n",
      "1938/1938 [==============================] - 1s 755us/step - loss: 620.0278 - mae: 19.8992 - val_loss: 745.7830 - val_mae: 21.5226\n",
      "Epoch 8/50\n",
      "1938/1938 [==============================] - 1s 764us/step - loss: 619.9816 - mae: 19.8925 - val_loss: 761.9482 - val_mae: 21.6006\n",
      "Epoch 9/50\n",
      "1938/1938 [==============================] - 1s 731us/step - loss: 620.0873 - mae: 19.8985 - val_loss: 741.7240 - val_mae: 21.5302\n",
      "Epoch 10/50\n",
      "1938/1938 [==============================] - 1s 716us/step - loss: 619.7540 - mae: 19.8863 - val_loss: 728.5186 - val_mae: 21.5178\n",
      "Epoch 11/50\n",
      "1938/1938 [==============================] - 1s 730us/step - loss: 620.0736 - mae: 19.8989 - val_loss: 737.5169 - val_mae: 21.5019\n",
      "Epoch 12/50\n",
      "1938/1938 [==============================] - 1s 729us/step - loss: 619.7534 - mae: 19.8846 - val_loss: 727.6289 - val_mae: 21.5119\n",
      "Epoch 13/50\n",
      "1938/1938 [==============================] - 2s 779us/step - loss: 620.0756 - mae: 19.8967 - val_loss: 736.3438 - val_mae: 21.5115\n",
      "Epoch 14/50\n",
      "1938/1938 [==============================] - 1s 766us/step - loss: 619.7032 - mae: 19.8873 - val_loss: 725.2277 - val_mae: 21.5273\n",
      "Epoch 15/50\n",
      "1938/1938 [==============================] - 1s 730us/step - loss: 619.7615 - mae: 19.8918 - val_loss: 753.2740 - val_mae: 21.5570\n",
      "Epoch 16/50\n",
      "1938/1938 [==============================] - 1s 741us/step - loss: 619.6125 - mae: 19.8829 - val_loss: 747.4786 - val_mae: 21.5236\n",
      "Epoch 17/50\n",
      "1938/1938 [==============================] - 1s 736us/step - loss: 619.6641 - mae: 19.8873 - val_loss: 741.8445 - val_mae: 21.5070\n",
      "Epoch 18/50\n",
      "1938/1938 [==============================] - 1s 772us/step - loss: 619.5650 - mae: 19.8819 - val_loss: 740.1188 - val_mae: 21.5181\n",
      "Epoch 19/50\n",
      "1938/1938 [==============================] - 2s 788us/step - loss: 619.4415 - mae: 19.8788 - val_loss: 731.0172 - val_mae: 21.5195\n",
      "Epoch 20/50\n",
      "1938/1938 [==============================] - 1s 714us/step - loss: 619.8256 - mae: 19.8886 - val_loss: 741.0211 - val_mae: 21.5201\n",
      "Epoch 21/50\n",
      "1938/1938 [==============================] - 1s 718us/step - loss: 619.5065 - mae: 19.8873 - val_loss: 743.7084 - val_mae: 21.5094\n",
      "Epoch 22/50\n",
      "1938/1938 [==============================] - 1s 723us/step - loss: 619.6438 - mae: 19.8803 - val_loss: 731.3206 - val_mae: 21.5038\n",
      "Epoch 23/50\n",
      "1938/1938 [==============================] - 1s 766us/step - loss: 619.4031 - mae: 19.8823 - val_loss: 743.2984 - val_mae: 21.5089\n",
      "Epoch 24/50\n",
      "1938/1938 [==============================] - 1s 736us/step - loss: 619.4901 - mae: 19.8836 - val_loss: 752.2938 - val_mae: 21.5351\n",
      "Epoch 25/50\n",
      "1938/1938 [==============================] - 1s 717us/step - loss: 619.4305 - mae: 19.8803 - val_loss: 741.1101 - val_mae: 21.5016\n",
      "Epoch 26/50\n",
      "1938/1938 [==============================] - 1s 751us/step - loss: 619.4814 - mae: 19.8829 - val_loss: 741.6720 - val_mae: 21.5074\n",
      "Epoch 27/50\n",
      "1938/1938 [==============================] - 1s 747us/step - loss: 619.5297 - mae: 19.8798 - val_loss: 745.2208 - val_mae: 21.5220\n",
      "Epoch 28/50\n",
      "1938/1938 [==============================] - 1s 731us/step - loss: 619.3039 - mae: 19.8845 - val_loss: 744.9542 - val_mae: 21.5170\n",
      "Epoch 29/50\n",
      "1938/1938 [==============================] - 1s 722us/step - loss: 619.4720 - mae: 19.8826 - val_loss: 744.4025 - val_mae: 21.5143\n",
      "Epoch 30/50\n",
      "1938/1938 [==============================] - 1s 724us/step - loss: 619.4233 - mae: 19.8815 - val_loss: 734.1661 - val_mae: 21.5014\n",
      "Epoch 31/50\n",
      "1938/1938 [==============================] - 1s 753us/step - loss: 619.2637 - mae: 19.8807 - val_loss: 736.3433 - val_mae: 21.4965\n",
      "Epoch 32/50\n",
      "1938/1938 [==============================] - 1s 755us/step - loss: 619.2814 - mae: 19.8797 - val_loss: 743.7039 - val_mae: 21.5172\n",
      "Epoch 33/50\n",
      "1938/1938 [==============================] - 1s 724us/step - loss: 619.2759 - mae: 19.8746 - val_loss: 732.6457 - val_mae: 21.5101\n",
      "Epoch 34/50\n",
      "1938/1938 [==============================] - 1s 749us/step - loss: 619.3511 - mae: 19.8855 - val_loss: 736.2922 - val_mae: 21.4946\n",
      "Epoch 35/50\n",
      "1938/1938 [==============================] - 1s 771us/step - loss: 619.2276 - mae: 19.8782 - val_loss: 753.3801 - val_mae: 21.5553\n",
      "Epoch 36/50\n",
      "1938/1938 [==============================] - 1s 731us/step - loss: 619.2347 - mae: 19.8738 - val_loss: 749.9486 - val_mae: 21.5293\n",
      "Epoch 37/50\n",
      "1938/1938 [==============================] - 1s 725us/step - loss: 619.2048 - mae: 19.8765 - val_loss: 742.3214 - val_mae: 21.5122\n",
      "Epoch 38/50\n",
      "1938/1938 [==============================] - 1s 720us/step - loss: 619.1588 - mae: 19.8727 - val_loss: 732.4006 - val_mae: 21.5030\n",
      "Epoch 39/50\n",
      "1938/1938 [==============================] - 1s 729us/step - loss: 619.3085 - mae: 19.8804 - val_loss: 742.8318 - val_mae: 21.5107\n",
      "Epoch 40/50\n",
      "1938/1938 [==============================] - 2s 777us/step - loss: 619.1981 - mae: 19.8740 - val_loss: 744.7453 - val_mae: 21.5138\n",
      "Epoch 41/50\n",
      "1938/1938 [==============================] - 1s 718us/step - loss: 619.1531 - mae: 19.8747 - val_loss: 743.7318 - val_mae: 21.5043\n",
      "Epoch 42/50\n",
      "1938/1938 [==============================] - 1s 741us/step - loss: 619.2237 - mae: 19.8710 - val_loss: 728.6436 - val_mae: 21.5071\n",
      "Epoch 43/50\n",
      "1938/1938 [==============================] - 1s 727us/step - loss: 619.2972 - mae: 19.8764 - val_loss: 731.3510 - val_mae: 21.5073\n",
      "Epoch 44/50\n",
      "1938/1938 [==============================] - 1s 732us/step - loss: 619.2001 - mae: 19.8736 - val_loss: 742.1219 - val_mae: 21.5123\n",
      "Epoch 45/50\n",
      "1938/1938 [==============================] - 1s 749us/step - loss: 619.0942 - mae: 19.8780 - val_loss: 744.1616 - val_mae: 21.5129\n",
      "Epoch 46/50\n",
      "1938/1938 [==============================] - 1s 768us/step - loss: 619.0725 - mae: 19.8682 - val_loss: 733.1316 - val_mae: 21.5044\n",
      "Epoch 47/50\n",
      "1938/1938 [==============================] - 1s 716us/step - loss: 618.9186 - mae: 19.8824 - val_loss: 767.6929 - val_mae: 21.6149\n",
      "Epoch 48/50\n",
      "1938/1938 [==============================] - 1s 707us/step - loss: 619.0561 - mae: 19.8673 - val_loss: 736.9744 - val_mae: 21.4929\n",
      "Epoch 49/50\n",
      "1938/1938 [==============================] - 1s 723us/step - loss: 619.2012 - mae: 19.8727 - val_loss: 737.4086 - val_mae: 21.4971\n",
      "Epoch 50/50\n",
      "1938/1938 [==============================] - 1s 743us/step - loss: 619.0164 - mae: 19.8719 - val_loss: 743.4504 - val_mae: 21.5191\n"
     ]
    }
   ],
   "source": [
    "hist10lemma = mm9.fit(X_ss_lemma, target_lemma, epochs=50, validation_split=.2)"
   ]
  }
 ]
}