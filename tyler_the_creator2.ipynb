{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd092ba50c92c9dc11b366869717e90d544d23b7140e20708921d0ff91f276d2e3f",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (main.py, line 21)",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\oguzk\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3437\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-ca1f4d9df417>\"\u001b[1;36m, line \u001b[1;32m20\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from main import *\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"c:\\Users\\oguzk\\NOVA\\Text Mining\\project\\main.py\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    def ngram_calc(pt, gt, mode=\"precision\", ngram):\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras import Sequential\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import string \n",
    "import re \n",
    "from nltk.tokenize import MWETokenizer, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(corpus, tool, language = 'en', pos_to_remove = ['PUNCT','NUM'], ent_to_remove = ['PERSON','ORG'], stop_words_to_remove= False, lowercase = True, regex=True):\n",
    "    \"\"\"\n",
    "    tool: one of two strings - 'spacy' or 'NLTK'\n",
    "    languages (string ISO code): supports 'en', 'fi' or 'zh'\n",
    "    pos_to_remove (list): part-of-speech tag from spacy\n",
    "    ent_to_remove (list): entities from spacy\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    tokenized_corpus = []\n",
    "    if tool == 'spacy':\n",
    "        if language == 'en':\n",
    "            sc = spacy.load('en_core_web_sm')\n",
    "            for doc in tqdm(sc.pipe(corpus, disable=[\"lemmatizer\", \"textcat\", \"custom\"])):\n",
    "                if stop_words_to_remove:\n",
    "                    doc_list = [word.text for word in doc if word.pos_ not in pos_to_remove if word.ent_type_ not in ent_to_remove if word.is_stop]\n",
    "                else: \n",
    "                    doc_list = [word.text for word in doc if word.pos_ not in pos_to_remove if word.ent_type_ not in ent_to_remove]\n",
    "\n",
    "                if lowercase:\n",
    "                    doc_list = [word.lower() for word in doc_list]\n",
    "\n",
    "                tokenized_corpus.append(doc_list)\n",
    "        if language == 'zh':\n",
    "            sc = spacy.load('zh_core_web_sm')\n",
    "            for doc in tqdm(sc.pipe(corpus, disable=[\"lemmatizer\", \"textcat\", \"custom\"])):\n",
    "                if stop_words_to_remove:\n",
    "                    doc_list = [word.text for word in doc if word.pos_ not in ['PUNCT'] if word.is_stop]\n",
    "                else: \n",
    "                    doc_list = [word.text for word in doc if word.pos_ not in ['PUNCT']]\n",
    "                \n",
    "                tokenized_corpus.append(doc_list)\n",
    "        if language == 'fi':\n",
    "            sc = spacy.load('xx_sent_ud_sm')\n",
    "            for doc in tqdm(sc.pipe(corpus, disable=[\"lemmatizer\", \"textcat\", \"custom\"])): \n",
    "                doc_list = [word.text for word in doc]\n",
    "                \n",
    "                if lowercase:\n",
    "                    doc_list = [word.lower() for word in doc_list]\n",
    "\n",
    "                tokenized_corpus.append(doc_list)\n",
    "                \n",
    "            \n",
    "        \n",
    "    if tool == 'NLTK':\n",
    "        print('Not implemented')\n",
    "        \n",
    "        \n",
    "    return tokenized_corpus\n",
    "\n",
    "\n",
    "def match_regex(tokenized_corpus, letters = True, letters_and_numbers = False):\n",
    "    \n",
    "    if letters:\n",
    "        regex = r'[a-z]+'\n",
    "    if letters_and_numbers:\n",
    "        regex = r'([a-z]+|^\\d+$)'\n",
    "        \n",
    "    new_tokenized = []\n",
    "    for sentence_list in tokenized_corpus:\n",
    "        sentence_list2 = [word for word in sentence_list if re.search(regex, word)]\n",
    "        new_tokenized.append(sentence_list2)\n",
    "        \n",
    "    return new_tokenized\n",
    "\n",
    "def chinese_regex(tokenized_corpus, chinese = True, chinese_and_numbers = False):\n",
    "    \n",
    "    if chinese:\n",
    "        regex = r'[\\u4e00-\\u9fff]+'\n",
    "    if chinese_and_numbers:\n",
    "        regex = r'([\\u4e00-\\u9fff]+|^\\d+$)'\n",
    "        \n",
    "    new_tokenized_zh = []\n",
    "    for sentence_list in tokenized_corpus:\n",
    "        chinese = [word for word in sentence_list if re.search(regex, word)]\n",
    "        new_tokenized_zh.append(chinese)\n",
    "        \n",
    "    return new_tokenized_zh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_preprocessor2(df,lang, tool=\"spacy\", pos_list=[], ent_list=[], stopwords=False, lowercase=True, let=True, letandnum=False):\n",
    "    if lang == \"en\":\n",
    "        word_vect = KeyedVectors.load('en_vectors.kv')\n",
    "\n",
    "        df[\"pt\"] = tokenization(df[\"translation\"], tool = tool, language = lang, pos_to_remove = pos_list, ent_to_remove = ent_list, stop_words_to_remove= stopwords, lowercase = lowercase)\n",
    "\n",
    "        df[\"ref\"] = tokenization(df[\"reference\"], tool = tool, language = lang, pos_to_remove = pos_list, ent_to_remove = ent_list, stop_words_to_remove= stopwords, lowercase = lowercase)\n",
    "\n",
    "    elif lang == \"fi\":\n",
    "        word_vect = KeyedVectors.load('fi_vectors.kv')\n",
    "\n",
    "        df[\"pt\"] = tokenization(df[\"translation\"], tool = tool, language = lang ,lowercase = lowercase)\n",
    "        df[\"pt\"] = match_regex(df[\"pt\"], letters = let, letters_and_numbers = letandnum)\n",
    "\n",
    "        df[\"ref\"] = tokenization(df[\"reference\"], tool = tool, language = lang, lowercase = lowercase)\n",
    "        df[\"ref\"] = match_regex(df[\"ref\"], letters = let, letters_and_numbers = letandnum)\n",
    "\n",
    "    elif lang == \"zh\":\n",
    "        word_vect = KeyedVectors.load('zh_vectors.kv')\n",
    "\n",
    "        df[\"pt\"] = tokenization(df[\"translation\"], tool = tool, language = lang, stop_words_to_remove= stopwords)\n",
    "\n",
    "        df[\"ref\"] = tokenization(df[\"reference\"], tool = tool, language = lang, stop_words_to_remove= stopwords)\n",
    "\n",
    "    else:\n",
    "        return \"ooppsiiee, language not defined\"\n",
    "\n",
    "    for mode in [\"precision\", \"recall\"]:\n",
    "        for i in range(1,5):\n",
    "            df[str(i)+\"gram-\"+ mode] = df.apply(lambda x: ngram_calc(x.pt, x.ref, mode=mode, ngram=i), axis =1)\n",
    "\n",
    "    df['wmdist'] = df.apply(lambda x: word_vect.wmdistance(x[\"pt\"],x[\"ref\"]), axis=1)\n",
    "    \n",
    "    df[\"pt_len\"] = df['pt'].str.len()\n",
    "    df[\"ref_len\"] = df['ref'].str.len()\n",
    "\n",
    "    df = df.drop([\"reference\", \"translation\", \"avg-score\", \"annotators\"], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_pairs = ['de-en', 'ru-en', 'zh-en']\n",
    "\n",
    "fi_df = pd.read_csv(\"corpus\\en-fi\\scores.csv\")\n",
    "zh_df = pd.read_csv(\"corpus\\en-zh\\scores.csv\")\n",
    "en_df = pd.read_csv(\"corpus\\cs-en\\scores.csv\")\n",
    "\n",
    "for pair in en_pairs:\n",
    "    new_df = pd.read_csv(os.path.join('corpus',pair, 'scores.csv'))\n",
    "    en_df = en_df.append(new_df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tokenization' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b2d1e1f1b691>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthe_preprocessor2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"spacy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PUNCT'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'NUM'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PERSON'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ORG'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mletandnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-c7f0416681fb>\u001b[0m in \u001b[0;36mthe_preprocessor2\u001b[1;34m(df, lang, tool, pos_list, ent_list, stopwords, lowercase, let, letandnum)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mword_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_vectors.kv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"translation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_to_remove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment_to_remove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ment_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words_to_remove\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ref\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reference\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_to_remove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment_to_remove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ment_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words_to_remove\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenization' is not defined"
     ]
    }
   ],
   "source": [
    "model1 = the_preprocessor2(en_df,\"en\", tool=\"spacy\", pos_list=['PUNCT','NUM'], ent_list=['PERSON','ORG'], stopwords=False, lowercase=True, let=True, letandnum=False)"
   ]
  }
 ]
}